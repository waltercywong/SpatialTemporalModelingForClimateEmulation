{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåé Welcome to the CSE151B Spring 2025 Climate Emulation Competition!\n",
    "\n",
    "Thank you for participating in this exciting challenge focused on building machine learning models to emulate complex climate systems.  \n",
    "This notebook is provided as a **starter template** to help you:\n",
    "\n",
    "- Understand how to load and preprocess the dataset  \n",
    "- Construct a baseline model  \n",
    "- Train and evaluate predictions using a PyTorch Lightning pipeline  \n",
    "- Format your predictions for submission to the leaderboard  \n",
    "\n",
    "You're encouraged to:\n",
    "- Build on this structure or replace it entirely\n",
    "- Try more advanced models and training strategies\n",
    "- Incorporate your own ideas to push the boundaries of what's possible\n",
    "\n",
    "If you're interested in developing within a repository structure and/or use helpful tools like configuration management (based on Hydra) and logging (with Weights & Biases), we recommend checking out the following Github repo. Such a structure can be useful when running multiple experiments and trying various research ideas.\n",
    "\n",
    "üëâ [https://github.com/salvaRC/cse151b-spring2025-competition](https://github.com/salvaRC/cse151b-spring2025-competition)\n",
    "\n",
    "Good luck, have fun, and we hope you learn a lot through this process!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶ Install Required Libraries\n",
    "We install the necessary Python packages for data loading, deep learning, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:41:18.548485Z",
     "iopub.status.busy": "2025-04-01T21:41:18.548240Z",
     "iopub.status.idle": "2025-04-01T21:41:22.118597Z",
     "shell.execute_reply": "2025-04-01T21:41:22.117410Z",
     "shell.execute_reply.started": "2025-04-01T21:41:18.548455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install xarray zarr dask lightning matplotlib wandb cftime einops --quiet\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning.pytorch as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è Configuration Setup  \n",
    "Define all model, data, and training hyperparameters in one place for easy control and reproducibility.\n",
    "\n",
    "### üìä Data Configuration\n",
    "\n",
    "We define the dataset settings used for training and evaluation. This includes:\n",
    "\n",
    "- **`path`**: Path to the `.zarr` dataset containing monthly climate variables from CMIP6 simulations.\n",
    "- **`input_vars`**: Climate forcing variables (e.g., CO‚ÇÇ, CH‚ÇÑ) used as model inputs.\n",
    "- **`output_vars`**: Target variables to predict ‚Äî surface air temperature (`tas`) and precipitation (`pr`).\n",
    "- **`target_member_id`**: Ensemble member to use from the simulations (each SSP has 3) for target variables.\n",
    "- **`train_ssps`**: SSP scenarios used for training (low to high emissions).\n",
    "- **`test_ssp`**: Scenario held out for evaluation (Must be set to SSP245).\n",
    "- **`test_months`**: Number of months to include in the test split (Must be set to 120).\n",
    "- **`batch_size`** and **`num_workers`**: Data loading parameters for PyTorch training.\n",
    "\n",
    "These settings reflect how the challenge is structured: models must learn from some emission scenarios and generalize to unseen ones.\n",
    "\n",
    "> ‚ö†Ô∏è **Important:** Do **not modify** the following test settings:\n",
    ">\n",
    "> - `test_ssp` must remain **`ssp245`**, which is the held-out evaluation scenario.\n",
    "> - `test_months` must be **`120`**, corresponding to the last 10 years (monthly resolution) of the scenario.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:41:44.343209Z",
     "iopub.status.busy": "2025-04-01T21:41:44.342908Z",
     "iopub.status.idle": "2025-04-01T21:41:44.359136Z",
     "shell.execute_reply": "2025-04-01T21:41:44.358296Z",
     "shell.execute_reply.started": "2025-04-01T21:41:44.343178Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE Change the data directory according to where you have your zarr files stored\n",
    "config = {\n",
    "    \"data\": {\n",
    "        # \"path\": \"/kaggle/input/cse151b-spring2025-competition/processed_data_cse151b_v2_corrupted_ssp245/processed_data_cse151b_v2_corrupted_ssp245.zarr\",\n",
    "        \"path\": \"/Users/walterwong/Desktop/CSE 151B/Project/processed_data_cse151b_v2_corrupted_ssp245.zarr\",\n",
    "        \"input_vars\": [\"CO2\", \"SO2\", \"CH4\", \"BC\", \"rsdt\"],\n",
    "        \"output_vars\": [\"tas\", \"pr\"],\n",
    "        \"target_member_id\": 0,\n",
    "        \"train_ssps\": [\"ssp126\", \"ssp370\", \"ssp585\"],\n",
    "        \"test_ssp\": \"ssp245\",\n",
    "        \"test_months\": 360,\n",
    "        \"batch_size\": 64,\n",
    "        \"num_workers\": 4,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"type\": \"simple_cnn\",\n",
    "        \"kernel_size\": 3,\n",
    "        \"init_dim\": 64,\n",
    "        \"depth\": 4,\n",
    "        \"dropout_rate\": 0.1,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 10,\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"devices\": \"auto\",\n",
    "        \"precision\": 32,\n",
    "        \"deterministic\": True,\n",
    "        \"num_sanity_val_steps\": 0,\n",
    "    },\n",
    "    \"seed\": 42,\n",
    "}\n",
    "pl.seed_everything(config[\"seed\"])  # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Spatial Weighting Utility Function\n",
    "\n",
    "This cell sets up utility functions for reproducibility and spatial weighting:\n",
    "\n",
    "- **`get_lat_weights(latitude_values)`**: Computes cosine-based area weights for each latitude, accounting for the Earth's curvature. This is critical for evaluating global climate metrics fairly ‚Äî grid cells near the equator represent larger surface areas than those near the poles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:41:47.931277Z",
     "iopub.status.busy": "2025-04-01T21:41:47.930982Z",
     "iopub.status.idle": "2025-04-01T21:41:47.935294Z",
     "shell.execute_reply": "2025-04-01T21:41:47.934417Z",
     "shell.execute_reply.started": "2025-04-01T21:41:47.931255Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_lat_weights(latitude_values):\n",
    "    lat_rad = np.deg2rad(latitude_values)\n",
    "    weights = np.cos(lat_rad)\n",
    "    return weights / np.mean(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† SimpleCNN: A Residual Convolutional Baseline\n",
    "\n",
    "This is a lightweight baseline model designed to capture spatial patterns in global climate data using convolutional layers.\n",
    "\n",
    "- The architecture starts with a **convolution + batch norm + ReLU** block to process the input channels.\n",
    "- It then applies a series of **residual blocks** to extract increasingly abstract spatial features. These help preserve gradient flow during training.\n",
    "- Finally, a few convolutional layers reduce the feature maps down to the desired number of output channels (`tas` and `pr`).\n",
    "\n",
    "This model only serves as a **simple baseline for climate emulation**. \n",
    "\n",
    "We encourage you to build and experiment with your own models and ideas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super().__init__()\n",
    "        # Increase kernel size for capturing larger spatial patterns in climate data\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=stride, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # Use LeakyReLU for better gradient flow with climate data\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.act(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.skip(identity)\n",
    "        return self.act(out)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, n_input_channels, n_output_channels, kernel_size=5, init_dim=128, depth=6, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        # Increased initial dimensions and depth for climate modeling capacity\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, init_dim, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm2d(init_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        current_dim = init_dim\n",
    "        for i in range(depth):\n",
    "            out_dim = current_dim * 2 if i < depth - 1 else current_dim\n",
    "            self.res_blocks.append(ResidualBlock(current_dim, out_dim))\n",
    "            if i < depth - 1:\n",
    "                current_dim *= 2\n",
    "                \n",
    "        # Added spatial attention for focusing on important climate patterns\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(current_dim, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        \n",
    "        # Modified final layers for better climate variable prediction\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(current_dim, current_dim // 2, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm2d(current_dim // 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(current_dim // 2, current_dim // 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(current_dim // 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(current_dim // 4, n_output_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "            \n",
    "        # Apply spatial attention\n",
    "        attention = self.spatial_attention(x)\n",
    "        x = x * attention\n",
    "        \n",
    "        return self.final(self.dropout(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:41:50.267367Z",
     "iopub.status.busy": "2025-04-01T21:41:50.267047Z",
     "iopub.status.idle": "2025-04-01T21:41:50.276495Z",
     "shell.execute_reply": "2025-04-01T21:41:50.275511Z",
     "shell.execute_reply.started": "2025-04-01T21:41:50.267321Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size // 2)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride), nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.skip(identity)\n",
    "        return self.relu(out)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, n_input_channels, n_output_channels, kernel_size=3, init_dim=64, depth=4, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, init_dim, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm2d(init_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        current_dim = init_dim\n",
    "        for i in range(depth):\n",
    "            out_dim = current_dim * 2 if i < depth - 1 else current_dim\n",
    "            self.res_blocks.append(ResidualBlock(current_dim, out_dim))\n",
    "            if i < depth - 1:\n",
    "                current_dim *= 2\n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(current_dim, current_dim // 2, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm2d(current_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(current_dim // 2, n_output_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "        return self.final(self.dropout(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìê Normalizer: Z-Score Scaling for Climate Inputs & Outputs\n",
    "\n",
    "This class handles **Z-score normalization**, a crucial preprocessing step for stable and efficient neural network training:\n",
    "\n",
    "- **`set_input_statistics(mean, std)` / `set_output_statistics(...)`**: Store the mean and standard deviation computed from the training data for later use.\n",
    "- **`normalize(data, data_type)`**: Standardizes the data using `(x - mean) / std`. This is applied separately to inputs and outputs.\n",
    "- **`inverse_transform_output(data)`**: Converts model predictions back to the original physical units (e.g., Kelvin for temperature, mm/day for precipitation).\n",
    "\n",
    "Normalizing the data ensures the model sees inputs with similar dynamic ranges and avoids biases caused by different variable scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:41:53.654123Z",
     "iopub.status.busy": "2025-04-01T21:41:53.653725Z",
     "iopub.status.idle": "2025-04-01T21:41:53.661660Z",
     "shell.execute_reply": "2025-04-01T21:41:53.660659Z",
     "shell.execute_reply.started": "2025-04-01T21:41:53.654087Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self):\n",
    "        self.mean_in, self.std_in = None, None\n",
    "        self.mean_out, self.std_out = None, None\n",
    "\n",
    "    def set_input_statistics(self, mean, std):\n",
    "        self.mean_in = mean\n",
    "        self.std_in = std\n",
    "\n",
    "    def set_output_statistics(self, mean, std):\n",
    "        self.mean_out = mean\n",
    "        self.std_out = std\n",
    "\n",
    "    def normalize(self, data, data_type):\n",
    "        if data_type == \"input\":\n",
    "            return (data - self.mean_in) / self.std_in\n",
    "        elif data_type == \"output\":\n",
    "            return (data - self.mean_out) / self.std_out\n",
    "\n",
    "    def inverse_transform_output(self, data):\n",
    "        return data * self.std_out + self.mean_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåç Data Module: Loading, Normalization, and Splitting\n",
    "\n",
    "This section handles the entire data pipeline, from loading the `.zarr` dataset to preparing PyTorch-ready DataLoaders.\n",
    "\n",
    "#### `ClimateDataset`\n",
    "- A simple PyTorch `Dataset` wrapper that preloads the entire (normalized) dataset into memory using Dask.\n",
    "- Converts the data to PyTorch tensors and handles any `NaN` checks up front.\n",
    "\n",
    "#### `ClimateDataModule`\n",
    "A PyTorch Lightning `DataModule` that handles:\n",
    "- ‚úÖ **Loading data** from different SSP scenarios and ensemble members\n",
    "- ‚úÖ **Broadcasting non-spatial inputs** (like CO‚ÇÇ) to match spatial grid size\n",
    "- ‚úÖ **Normalization** using mean/std computed from training data only\n",
    "- ‚úÖ **Splitting** into training, validation, and test sets:\n",
    "  - Training: All months from selected SSPs (except last 10 years of SSP370)\n",
    "  - Validation: Last 10 years (120 months) of SSP370\n",
    "  - Test: Last 10 years of SSP245 (unseen scenario)\n",
    "- ‚úÖ **Batching** and parallelized data loading via PyTorch `DataLoader`s\n",
    "- ‚úÖ **Latitude-based area weighting** for fair climate metric evaluation\n",
    "- Shape of the inputs are Batch_Size X 5 (num_input_variables) X 48 X 72\n",
    "- Shape of ouputputs are Batch_Size X 2 (num_output_variables) X 48 X 72\n",
    "\n",
    "> ‚ÑπÔ∏è **Note:** You likely won‚Äôt need to modify this class but feel free to make modifications if you want to inlcude different ensemble mebers to feed more data to your models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:47:19.571900Z",
     "iopub.status.busy": "2025-04-01T21:47:19.571562Z",
     "iopub.status.idle": "2025-04-01T21:47:19.591655Z",
     "shell.execute_reply": "2025-04-01T21:47:19.590706Z",
     "shell.execute_reply.started": "2025-04-01T21:47:19.571870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, inputs_dask, outputs_dask, output_is_normalized=True):\n",
    "        self.size = inputs_dask.shape[0]\n",
    "        print(f\"Creating dataset with {self.size} samples...\")\n",
    "\n",
    "        inputs_np = inputs_dask.compute()\n",
    "        outputs_np = outputs_dask.compute()\n",
    "\n",
    "        self.inputs = torch.from_numpy(inputs_np).float()\n",
    "        self.outputs = torch.from_numpy(outputs_np).float()\n",
    "\n",
    "        if torch.isnan(self.inputs).any() or torch.isnan(self.outputs).any():\n",
    "            raise ValueError(\"NaNs found in dataset\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "\n",
    "class ClimateDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path,\n",
    "        input_vars,\n",
    "        output_vars,\n",
    "        train_ssps,\n",
    "        test_ssp,\n",
    "        target_member_id,\n",
    "        val_split=0.1,\n",
    "        test_months=120,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        seed=42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.input_vars = input_vars\n",
    "        self.output_vars = output_vars\n",
    "        self.train_ssps = train_ssps\n",
    "        self.test_ssp = test_ssp\n",
    "        self.target_member_id = target_member_id\n",
    "        self.val_split = val_split\n",
    "        self.test_months = test_months\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.normalizer = Normalizer()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        assert os.path.exists(self.path), f\"Data path not found: {self.path}\"\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        ds = xr.open_zarr(self.path, consolidated=False, chunks={\"time\": 24})\n",
    "        spatial_template = ds[\"rsdt\"].isel(time=0, ssp=0, drop=True)\n",
    "\n",
    "        def load_ssp(ssp):\n",
    "            input_dask, output_dask = [], []\n",
    "            for var in self.input_vars:\n",
    "                da_var = ds[var].sel(ssp=ssp)\n",
    "                if \"latitude\" in da_var.dims:\n",
    "                    da_var = da_var.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n",
    "                if \"member_id\" in da_var.dims:\n",
    "                    da_var = da_var.sel(member_id=self.target_member_id)\n",
    "                if set(da_var.dims) == {\"time\"}:\n",
    "                    da_var = da_var.broadcast_like(spatial_template).transpose(\"time\", \"y\", \"x\")\n",
    "                input_dask.append(da_var.data)\n",
    "\n",
    "            for var in self.output_vars:\n",
    "                da_out = ds[var].sel(ssp=ssp, member_id=self.target_member_id)\n",
    "                if \"latitude\" in da_out.dims:\n",
    "                    da_out = da_out.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n",
    "                output_dask.append(da_out.data)\n",
    "\n",
    "            return da.stack(input_dask, axis=1), da.stack(output_dask, axis=1)\n",
    "\n",
    "        train_input, train_output, val_input, val_output = [], [], None, None\n",
    "\n",
    "        for ssp in self.train_ssps:\n",
    "            x, y = load_ssp(ssp)\n",
    "            if ssp == \"ssp370\":\n",
    "                val_input = x[-self.test_months:]\n",
    "                val_output = y[-self.test_months:]\n",
    "                train_input.append(x[:-self.test_months])\n",
    "                train_output.append(y[:-self.test_months])\n",
    "            else:\n",
    "                train_input.append(x)\n",
    "                train_output.append(y)\n",
    "\n",
    "        train_input = da.concatenate(train_input, axis=0)\n",
    "        train_output = da.concatenate(train_output, axis=0)\n",
    "\n",
    "        self.normalizer.set_input_statistics(\n",
    "            mean=da.nanmean(train_input, axis=(0, 2, 3), keepdims=True).compute(),\n",
    "            std=da.nanstd(train_input, axis=(0, 2, 3), keepdims=True).compute(),\n",
    "        )\n",
    "        self.normalizer.set_output_statistics(\n",
    "            mean=da.nanmean(train_output, axis=(0, 2, 3), keepdims=True).compute(),\n",
    "            std=da.nanstd(train_output, axis=(0, 2, 3), keepdims=True).compute(),\n",
    "        )\n",
    "\n",
    "        train_input_norm = self.normalizer.normalize(train_input, \"input\")\n",
    "        train_output_norm = self.normalizer.normalize(train_output, \"output\")\n",
    "        val_input_norm = self.normalizer.normalize(val_input, \"input\")\n",
    "        val_output_norm = self.normalizer.normalize(val_output, \"output\")\n",
    "\n",
    "        test_input, test_output = load_ssp(self.test_ssp)\n",
    "        test_input = test_input[-self.test_months:]\n",
    "        test_output = test_output[-self.test_months:]\n",
    "        test_input_norm = self.normalizer.normalize(test_input, \"input\")\n",
    "\n",
    "        self.train_dataset = ClimateDataset(train_input_norm, train_output_norm)\n",
    "        self.val_dataset = ClimateDataset(val_input_norm, val_output_norm)\n",
    "        self.test_dataset = ClimateDataset(test_input_norm, test_output, output_is_normalized=False)\n",
    "\n",
    "        self.lat = spatial_template.y.values\n",
    "        self.lon = spatial_template.x.values\n",
    "        self.area_weights = xr.DataArray(get_lat_weights(self.lat), dims=[\"y\"], coords={\"y\": self.lat})\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def get_lat_weights(self):\n",
    "        return self.area_weights\n",
    "\n",
    "    def get_coords(self):\n",
    "        return self.lat, self.lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° ClimateEmulationModule: Lightning Wrapper for Climate Model Emulation\n",
    "\n",
    "This is the core model wrapper built with **PyTorch Lightning**, which organizes the training, validation, and testing logic for the climate emulation task. Lightning abstracts away much of the boilerplate code in PyTorch-based deep learning workflows, making it easier to scale models.\n",
    "\n",
    "#### ‚úÖ Key Features\n",
    "\n",
    "- **`training_step` / `validation_step` / `test_step`**: Standard Lightning hooks for computing loss and predictions at each stage. The loss used is **Mean Squared Error (MSE)**.\n",
    "\n",
    "- **Normalization-aware outputs**:\n",
    "  - During validation and testing, predictions and targets are denormalized before evaluation using stored mean/std statistics.\n",
    "  - This ensures evaluation is done in real-world units (Kelvin and mm/day).\n",
    "\n",
    "- **Metric Evaluation** via `_evaluate()`:\n",
    "  For each variable (`tas`, `pr`), it calculates:\n",
    "  - **Monthly Area-Weighted RMSE**\n",
    "  - **Time-Mean RMSE** (RMSE on 10-year average's)\n",
    "  - **Time-Stddev MAE** (MAE on 10-year standard deviation; a measure of temporal variability)\n",
    "    \n",
    "  These metrics reflect the competition's evaluation criteria and are logged and printed.\n",
    "\n",
    "- **Kaggle Submission Writer**:\n",
    "  After testing, predictions are saved to a `.csv` file in the required Kaggle format via `_save_submission()`.\n",
    "\n",
    "- **Saving Predictions for Visualization**:\n",
    "  - Validation predictions are saved tao `val_preds.npy` and `val_trues.npy`\n",
    "  - These can be loaded later for visual inspection of the model's performance.\n",
    "\n",
    " üîß **Feel free to modify any part of this module** (loss functions, evaluation, training logic) to better suit your model or training pipeline / Use pure PyTorch etc.\n",
    "\n",
    "‚ö†Ô∏è The **final submission `.csv` file must strictly follow the format and naming convention used in `_save_submission()`**, as these `ID`s are used to match predictions to the hidden test set during evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    \"\"\"Applies a module over multiple time steps\"\"\"\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape to (batch_size * time_steps, ...)\n",
    "        batch_size, time_steps = x.size(0), x.size(1)\n",
    "        x_reshaped = x.contiguous().view(batch_size * time_steps, *x.size()[2:])\n",
    "        \n",
    "        # Apply module\n",
    "        y = self.module(x_reshaped)\n",
    "        \n",
    "        # Reshape back\n",
    "        return y.contiguous().view(batch_size, time_steps, *y.size()[1:])\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial attention mechanism for focusing on important regions\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate attention map\n",
    "        attention = torch.sigmoid(self.conv(x))\n",
    "        return x * attention\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"Temporal attention mechanism for focusing on important time steps\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time_steps, features]\n",
    "        # Generate attention weights\n",
    "        attention_weights = F.softmax(self.fc(x), dim=1)\n",
    "        # Apply attention\n",
    "        context = torch.sum(x * attention_weights, dim=1)\n",
    "        return context\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with larger kernels for climate patterns\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, \n",
    "                              stride=stride, padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n",
    "                              stride=1, padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection if dimensions change\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class TimeSeriesClimateCNN(nn.Module):\n",
    "    \"\"\"CNN model for climate time series prediction with optional LSTM\"\"\"\n",
    "    def __init__(self, n_input_channels, n_output_channels, seq_length=12, \n",
    "                 kernel_size=5, init_dim=64, depth=4, dropout_rate=0.3, use_lstm=True):\n",
    "        super(TimeSeriesClimateCNN, self).__init__()\n",
    "        \n",
    "        self.n_input_channels = n_input_channels\n",
    "        self.n_output_channels = n_output_channels\n",
    "        self.seq_length = seq_length\n",
    "        self.use_lstm = use_lstm\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        self.initial_conv = TimeDistributed(\n",
    "            nn.Conv2d(n_input_channels, init_dim, kernel_size=kernel_size, \n",
    "                     stride=1, padding=kernel_size//2)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks with increasing channels\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        channels = [init_dim * (2**i) for i in range(depth)]\n",
    "        \n",
    "        for i in range(depth-1):\n",
    "            block = TimeDistributed(\n",
    "                ResidualBlock(channels[i], channels[i+1], kernel_size=kernel_size, stride=2)\n",
    "            )\n",
    "            self.res_blocks.append(block)\n",
    "        \n",
    "        # Spatial attention after CNN layers\n",
    "        self.spatial_attention = TimeDistributed(SpatialAttention(channels[-1]))\n",
    "        \n",
    "        # Global average pooling to reduce spatial dimensions\n",
    "        self.gap = TimeDistributed(nn.AdaptiveAvgPool2d(1))\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        if use_lstm:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=channels[-1],\n",
    "                hidden_size=channels[-1],\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout_rate,\n",
    "                bidirectional=True\n",
    "            )\n",
    "            lstm_output_dim = channels[-1] * 2  # bidirectional\n",
    "        else:\n",
    "            lstm_output_dim = channels[-1]\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.temporal_attention = TemporalAttention(lstm_output_dim)\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Upsampling to original resolution\n",
    "        self.upsampling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(lstm_output_dim, channels[-2], kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(channels[-2], channels[-3], kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(channels[-3], n_output_channels, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time_steps, channels, height, width]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Apply initial convolution\n",
    "        x = self.initial_conv(x)\n",
    "        \n",
    "        # Apply residual blocks\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        x = self.spatial_attention(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.gap(x)\n",
    "        x = x.view(batch_size, self.seq_length, -1)  # [batch, time_steps, channels]\n",
    "        \n",
    "        # Apply LSTM if enabled\n",
    "        if self.use_lstm:\n",
    "            x, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply temporal attention to focus on important time steps\n",
    "        x = self.temporal_attention(x)  # [batch, channels]\n",
    "        \n",
    "        # Reshape for upsampling\n",
    "        x = x.view(batch_size, -1, 1, 1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Upsampling to get final prediction\n",
    "        x = self.upsampling(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ClimateTransformerCNN(nn.Module):\n",
    "    \"\"\"CNN model with transformer for temporal modeling\"\"\"\n",
    "    def __init__(self, n_input_channels, n_output_channels, seq_length=12, \n",
    "                 kernel_size=5, init_dim=64, depth=4, n_heads=8, dropout_rate=0.3):\n",
    "        super(ClimateTransformerCNN, self).__init__()\n",
    "        \n",
    "        self.n_input_channels = n_input_channels\n",
    "        self.n_output_channels = n_output_channels\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        self.initial_conv = TimeDistributed(\n",
    "            nn.Conv2d(n_input_channels, init_dim, kernel_size=kernel_size, \n",
    "                     stride=1, padding=kernel_size//2)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks with increasing channels\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        channels = [init_dim * (2**i) for i in range(depth)]\n",
    "        \n",
    "        for i in range(depth-1):\n",
    "            block = TimeDistributed(\n",
    "                ResidualBlock(channels[i], channels[i+1], kernel_size=kernel_size, stride=2)\n",
    "            )\n",
    "            self.res_blocks.append(block)\n",
    "        \n",
    "        # Spatial attention after CNN layers\n",
    "        self.spatial_attention = TimeDistributed(SpatialAttention(channels[-1]))\n",
    "        \n",
    "        # Global average pooling to reduce spatial dimensions\n",
    "        self.gap = TimeDistributed(nn.AdaptiveAvgPool2d(1))\n",
    "        \n",
    "        # Transformer encoder for temporal modeling\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=channels[-1],\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=channels[-1]*4,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.temporal_attention = TemporalAttention(channels[-1])\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Upsampling to original resolution\n",
    "        self.upsampling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(channels[-1], channels[-2], kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(channels[-2], channels[-3], kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(channels[-3], n_output_channels, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time_steps, channels, height, width]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Apply initial convolution\n",
    "        x = self.initial_conv(x)\n",
    "        \n",
    "        # Apply residual blocks\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        x = self.spatial_attention(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.gap(x)\n",
    "        x = x.view(batch_size, self.seq_length, -1)  # [batch, time_steps, channels]\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Apply temporal attention to focus on important time steps\n",
    "        x = self.temporal_attention(x)  # [batch, channels]\n",
    "        \n",
    "        # Reshape for upsampling\n",
    "        x = x.view(batch_size, -1, 1, 1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Upsampling to get final prediction\n",
    "        x = self.upsampling(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:47:23.669376Z",
     "iopub.status.busy": "2025-04-01T21:47:23.669041Z",
     "iopub.status.idle": "2025-04-01T21:47:23.684782Z",
     "shell.execute_reply": "2025-04-01T21:47:23.683857Z",
     "shell.execute_reply.started": "2025-04-01T21:47:23.669319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class ClimateEmulationModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.save_hyperparameters(ignore=['model']) # Save all hyperparameters except the model to self.hparams.<param_name>\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.normalizer = None\n",
    "        self.val_preds, self.val_targets = [], []\n",
    "        self.test_preds, self.test_targets = [], []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        self.normalizer = self.trainer.datamodule.normalizer  # Get the normalizer from the datamodule (see above)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch # Unpack inputs and targets (this is the output of the _getitem_ method in the Dataset above)\n",
    "        y_hat = self(x)   # Forward pass\n",
    "        loss = self.criterion(y_hat, y)  # Calculate loss\n",
    "        self.log(\"train/loss\", loss)  # Log loss for tracking\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val/loss\", loss)\n",
    "\n",
    "        y_hat_np = self.normalizer.inverse_transform_output(y_hat.detach().cpu().numpy())\n",
    "        y_np = self.normalizer.inverse_transform_output(y.detach().cpu().numpy())\n",
    "        self.val_preds.append(y_hat_np)\n",
    "        self.val_targets.append(y_np)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Concatenate all predictions and ground truths from each val step/batch into one array\n",
    "        preds = np.concatenate(self.val_preds, axis=0)\n",
    "        trues = np.concatenate(self.val_targets, axis=0)\n",
    "        self._evaluate(preds, trues, phase=\"val\")\n",
    "        np.save(\"val_preds.npy\", preds)\n",
    "        np.save(\"val_trues.npy\", trues)\n",
    "        self.val_preds.clear()\n",
    "        self.val_targets.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat_np = self.normalizer.inverse_transform_output(y_hat.detach().cpu().numpy())\n",
    "        y_np = y.detach().cpu().numpy()\n",
    "        self.test_preds.append(y_hat_np)\n",
    "        self.test_targets.append(y_np)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Concatenate all predictions and ground truths from each test step/batch into one array\n",
    "        preds = np.concatenate(self.test_preds, axis=0)\n",
    "        trues = np.concatenate(self.test_targets, axis=0)\n",
    "        self._evaluate(preds, trues, phase=\"test\")\n",
    "        self._save_submission(preds)\n",
    "        self.test_preds.clear()\n",
    "        self.test_targets.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "    def _evaluate(self, preds, trues, phase=\"val\"):\n",
    "        datamodule = self.trainer.datamodule\n",
    "        area_weights = datamodule.get_lat_weights()\n",
    "        lat, lon = datamodule.get_coords()\n",
    "        time = np.arange(preds.shape[0])\n",
    "        output_vars = datamodule.output_vars\n",
    "\n",
    "        for i, var in enumerate(output_vars):\n",
    "            p = preds[:, i]\n",
    "            t = trues[:, i]\n",
    "            p_xr = xr.DataArray(p, dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n",
    "            t_xr = xr.DataArray(t, dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n",
    "\n",
    "            # RMSE\n",
    "            rmse = np.sqrt(((p_xr - t_xr) ** 2).weighted(area_weights).mean((\"time\", \"y\", \"x\")).item())\n",
    "            # RMSE of time-mean\n",
    "            mean_rmse = np.sqrt(((p_xr.mean(\"time\") - t_xr.mean(\"time\")) ** 2).weighted(area_weights).mean((\"y\", \"x\")).item())\n",
    "            # MAE of time-stddev\n",
    "            std_mae = np.abs(p_xr.std(\"time\") - t_xr.std(\"time\")).weighted(area_weights).mean((\"y\", \"x\")).item()\n",
    "\n",
    "            print(f\"[{phase.upper()}] {var}: RMSE={rmse:.4f}, Time-Mean RMSE={mean_rmse:.4f}, Time-Stddev MAE={std_mae:.4f}\")\n",
    "            self.log_dict({\n",
    "                f\"{phase}/{var}/rmse\": rmse,\n",
    "                f\"{phase}/{var}/time_mean_rmse\": mean_rmse,\n",
    "                f\"{phase}/{var}/time_std_mae\": std_mae,\n",
    "            })\n",
    "\n",
    "    def _save_submission(self, predictions):\n",
    "        datamodule = self.trainer.datamodule\n",
    "        lat, lon = datamodule.get_coords()\n",
    "        output_vars = datamodule.output_vars\n",
    "        time = np.arange(predictions.shape[0])\n",
    "\n",
    "        rows = []\n",
    "        for t_idx, t in enumerate(time):\n",
    "            for var_idx, var in enumerate(output_vars):\n",
    "                for y_idx, y in enumerate(lat):\n",
    "                    for x_idx, x in enumerate(lon):\n",
    "                        row_id = f\"t{t_idx:03d}_{var}_{y:.2f}_{x:.2f}\"\n",
    "                        pred = predictions[t_idx, var_idx, y_idx, x_idx]\n",
    "                        rows.append({\"ID\": row_id, \"Prediction\": pred})\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "        os.makedirs(\"submissions\", exist_ok=True)\n",
    "        filepath = f\"submissions/kaggle_submission_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"‚úÖ Submission saved to: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ö° Training & Evaluation with PyTorch Lightning\n",
    "\n",
    "This block sets up and runs the training and testing pipeline using **PyTorch Lightning‚Äôs `Trainer`**, which abstracts away much of the boilerplate in deep learning workflows.\n",
    "\n",
    "- **Modular Setup**:\n",
    "  - `datamodule`: Handles loading, normalization, and batching of climate data.\n",
    "  - `model`: A convolutional neural network that maps climate forcings to predicted outputs.\n",
    "  - `lightning_module`: Wraps the model with training/validation/test logic and metric evaluation.\n",
    "\n",
    "- **Trainer Flexibility**:\n",
    "  The `Trainer` accepts a wide range of configuration options from `config[\"trainer\"]`, including:\n",
    "  - Number of epochs\n",
    "  - Precision (e.g., 16-bit or 32-bit)\n",
    "  - Device configuration (CPU, GPU, or TPU)\n",
    "  - Determinism, logging, callbacks, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load data first to check dimensions\n",
    "# datamodule = ClimateDataModule(**config[\"data\"])\n",
    "# datamodule.setup(\"fit\")  # This will load the data and prepare it\n",
    "\n",
    "# # Get input dimensions from the actual data\n",
    "# sample_batch = next(iter(datamodule.train_dataloader()))\n",
    "# input_sample, _ = sample_batch\n",
    "# print(f\"Input shape: {input_sample.shape}\")  # Should show [batch_size, seq_length, channels, height, width]\n",
    "\n",
    "# # Create model with correct dimensions\n",
    "# model = None\n",
    "# if config[\"model\"][\"type\"] == \"cnn_transformer\":\n",
    "#     model = ClimateTransformerCNN(\n",
    "#         n_input_channels=input_sample.shape[2],  # Use actual channel count from data\n",
    "#         n_output_channels=len(config[\"data\"][\"output_vars\"]),\n",
    "#         seq_length=input_sample.shape[1],  # Use actual sequence length from data\n",
    "#         kernel_size=config[\"model\"].get(\"kernel_size\", 5),\n",
    "#         init_dim=config[\"model\"].get(\"init_dim\", 64),\n",
    "#         depth=config[\"model\"].get(\"depth\", 4),\n",
    "#         n_heads=config[\"model\"].get(\"n_heads\", 8),\n",
    "#         dropout_rate=config[\"model\"].get(\"dropout_rate\", 0.3)\n",
    "#     )\n",
    "# else:\n",
    "#     model = TimeSeriesClimateCNN(\n",
    "#         n_input_channels=input_sample.shape[2],  # Use actual channel count from data\n",
    "#         n_output_channels=len(config[\"data\"][\"output_vars\"]),\n",
    "#         seq_length=input_sample.shape[1],  # Use actual sequence length from data\n",
    "#         kernel_size=config[\"model\"].get(\"kernel_size\", 5),\n",
    "#         init_dim=config[\"model\"].get(\"init_dim\", 64),\n",
    "#         depth=config[\"model\"].get(\"depth\", 4),\n",
    "#         dropout_rate=config[\"model\"].get(\"dropout_rate\", 0.3)\n",
    "#     )\n",
    "\n",
    "# lightning_module = ClimateEmulationModule(model, learning_rate=config[\"training\"][\"lr\"])\n",
    "\n",
    "# trainer = pl.Trainer(**config[\"trainer\"])\n",
    "# try:\n",
    "#     trainer.fit(lightning_module, datamodule=datamodule)   # Training\n",
    "# except KeyboardInterrupt:\n",
    "#     print(\"Training interrupted by user\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error during training: {e}\")\n",
    "#     import traceback\n",
    "#     traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 2703 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type                 | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model     | SequentialClimateCNN | 1.3 M  | train\n",
      "1 | criterion | MSELoss              | 0      | train\n",
      "-----------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.036     Total estimated model params size (MB)\n",
      "45        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 360 samples...\n",
      "Creating dataset with 360 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n",
      "python(88794) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(88795) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(88796) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(88797) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (43) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/43 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    exitcode = _main(fd, parent_sentinel)    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: \n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    Can't get attribute 'ClimateDataset' on <module '__main__' (built-in)>\n",
      "self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'ClimateDataset' on <module '__main__' (built-in)>\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'ClimateDataset' on <module '__main__' (built-in)>    self = reduction.pickle.load(from_parent)\n",
      "\n",
      "AttributeError: Can't get attribute 'ClimateDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during training: DataLoader worker (pid(s) 88794, 88795, 88796, 88797) exited unexpectedly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1133, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/multiprocessing/queues.py\", line 114, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/75/4b4y2mlx2yv_34rkxb83ydkr0000gn/T/ipykernel_82012/3482048120.py\", line 42, in <module>\n",
      "    trainer.fit(lightning_module, datamodule=datamodule)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1056, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 150, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 282, in advance\n",
      "    batch, _, __ = next(data_fetcher)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/loops/fetchers.py\", line 134, in __next__\n",
      "    batch = super().__next__()\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/loops/fetchers.py\", line 61, in __next__\n",
      "    batch = next(self.iterator)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 341, in __next__\n",
      "    out = next(self._iterator)\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/pytorch_lightning/utilities/combined_loader.py\", line 78, in __next__\n",
      "    out[i] = next(self.iterators[i])\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1329, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1295, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/Users/walterwong/miniforge3/envs/project_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\", line 1146, in _try_get_data\n",
      "    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\n",
      "RuntimeError: DataLoader worker (pid(s) 88794, 88795, 88796, 88797) exited unexpectedly\n"
     ]
    }
   ],
   "source": [
    "from models import SequentialClimateCNN\n",
    "\n",
    "datamodule = ClimateDataModule(**config[\"data\"])\n",
    "model = SequentialClimateCNN(\n",
    "    n_input_channels=len(config[\"data\"][\"input_vars\"]),\n",
    "    n_output_channels=len(config[\"data\"][\"output_vars\"]),\n",
    "    seq_length=config[\"data\"].get(\"seq_length\", 12),\n",
    "    kernel_size=config[\"model\"].get(\"kernel_size\", 5),\n",
    "    hidden_dim=config[\"model\"].get(\"init_dim\", 64),\n",
    "    spatial_depth=config[\"model\"].get(\"depth\", 3),\n",
    "    dropout_rate=config[\"model\"].get(\"dropout_rate\", 0.3)\n",
    ")\n",
    "lightning_module = ClimateEmulationModule(model, learning_rate=config[\"training\"][\"lr\"])\n",
    "\n",
    "trainer = pl.Trainer(**config[\"trainer\"])\n",
    "trainer.fit(lightning_module, datamodule=datamodule)   #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:47:27.937745Z",
     "iopub.status.busy": "2025-04-01T21:47:27.937440Z",
     "iopub.status.idle": "2025-04-01T21:54:19.582044Z",
     "shell.execute_reply": "2025-04-01T21:54:19.581285Z",
     "shell.execute_reply.started": "2025-04-01T21:47:27.937722Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "datamodule = ClimateDataModule(**config[\"data\"])\n",
    "model = SimpleCNN(\n",
    "    n_input_channels=len(config[\"data\"][\"input_vars\"]),\n",
    "    n_output_channels=len(config[\"data\"][\"output_vars\"]),\n",
    "    **{k: v for k, v in config[\"model\"].items() if k != \"type\"}\n",
    ")\n",
    "lightning_module = ClimateEmulationModule(model, learning_rate=config[\"training\"][\"lr\"])\n",
    "\n",
    "trainer = pl.Trainer(**config[\"trainer\"])\n",
    "trainer.fit(lightning_module, datamodule=datamodule)   # Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model\n",
    "\n",
    "**IMPORTANT:** Please note that the test metrics will be bad because the test targets have been corrupted on the public Kaggle dataset.\n",
    "The purpose of testing below is to generate the Kaggle submission file based on your model's predictions, which you can submit to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:54:22.622589Z",
     "iopub.status.busy": "2025-04-01T21:54:22.622213Z",
     "iopub.status.idle": "2025-04-01T21:54:34.498363Z",
     "shell.execute_reply": "2025-04-01T21:54:34.497464Z",
     "shell.execute_reply.started": "2025-04-01T21:54:22.622553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.test(lightning_module, datamodule=datamodule) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:54:40.424172Z",
     "iopub.status.busy": "2025-04-01T21:54:40.423863Z",
     "iopub.status.idle": "2025-04-01T21:54:40.430857Z",
     "shell.execute_reply": "2025-04-01T21:54:40.429891Z",
     "shell.execute_reply.started": "2025-04-01T21:54:40.424147Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plot_comparison(true_xr, pred_xr, title, cmap='viridis', diff_cmap='RdBu_r', metric=None):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    vmin = min(true_xr.min().item(), pred_xr.min().item())\n",
    "    vmax = max(true_xr.max().item(), pred_xr.max().item())\n",
    "\n",
    "    # Ground truth\n",
    "    true_xr.plot(ax=axs[0], cmap=cmap, vmin=vmin, vmax=vmax, add_colorbar=True)\n",
    "    axs[0].set_title(f\"{title} (Ground Truth)\")\n",
    "\n",
    "    # Prediction\n",
    "    pred_xr.plot(ax=axs[1], cmap=cmap, vmin=vmin, vmax=vmax, add_colorbar=True)\n",
    "    axs[1].set_title(f\"{title} (Prediction)\")\n",
    "\n",
    "    # Difference\n",
    "    diff = pred_xr - true_xr\n",
    "    abs_max = np.max(np.abs(diff))\n",
    "    diff.plot(ax=axs[2], cmap=diff_cmap, vmin=-abs_max, vmax=abs_max, add_colorbar=True)\n",
    "    axs[2].set_title(f\"{title} (Difference) {f'- {metric:.4f}' if metric else ''}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñºÔ∏è Visualizing Validation Predictions\n",
    "\n",
    "This cell loads saved validation predictions and compares them to the ground truth using spatial plots. These visualizations help you qualitatively assess your model's performance.\n",
    "\n",
    "For each output variable (`tas`, `pr`), we visualize:\n",
    "\n",
    "- **üìà Time-Mean Map**: The 10-year average spatial pattern for both prediction and ground truth. Helps identify long-term biases or spatial shifts.\n",
    "- **üìä Time-Stddev Map**: Shows the standard deviation across time for each grid cell ‚Äî useful for assessing how well the model captures **temporal variability** at each location.\n",
    "- **üïì Random Timestep Sample**: Visual comparison of prediction vs ground truth for a single month. Useful for spotting fine-grained anomalies or errors in specific months.\n",
    "\n",
    "> These plots provide intuition beyond metrics and are useful for debugging spatial or temporal model failures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-01T21:54:42.413739Z",
     "iopub.status.busy": "2025-04-01T21:54:42.413433Z",
     "iopub.status.idle": "2025-04-01T21:54:46.942960Z",
     "shell.execute_reply": "2025-04-01T21:54:46.942037Z",
     "shell.execute_reply.started": "2025-04-01T21:54:42.413716Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load validation predictions\n",
    "# make sure to have run the validation loop at least once\n",
    "val_preds = np.load(\"val_preds.npy\")\n",
    "val_trues = np.load(\"val_trues.npy\")\n",
    "\n",
    "lat, lon = datamodule.get_coords()\n",
    "output_vars = datamodule.output_vars\n",
    "time = np.arange(val_preds.shape[0])\n",
    "\n",
    "for i, var in enumerate(output_vars):\n",
    "    pred_xr = xr.DataArray(val_preds[:, i], dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n",
    "    true_xr = xr.DataArray(val_trues[:, i], dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n",
    "\n",
    "    # --- Time Mean ---\n",
    "    plot_comparison(true_xr.mean(\"time\"), pred_xr.mean(\"time\"), f\"{var} Val Time-Mean\")\n",
    "\n",
    "    # --- Time Stddev ---\n",
    "    plot_comparison(true_xr.std(\"time\"), pred_xr.std(\"time\"), f\"{var} Val Time-Stddev\", cmap=\"plasma\")\n",
    "\n",
    "    # --- Random timestep ---\n",
    "    t_idx = np.random.randint(0, len(time))\n",
    "    plot_comparison(true_xr.isel(time=t_idx), pred_xr.isel(time=t_idx), f\"{var} Val Sample Timestep {t_idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß™ Final Notes\n",
    "\n",
    "This notebook is meant to serve as a **baseline template** ‚Äî a starting point to help you get up and running quickly with the climate emulation challenge.\n",
    "\n",
    "You are **not** required to stick to this exact setup. In fact, we **encourage** you to:\n",
    "\n",
    "- üîÅ Build on top of the provided `DataModule`. \n",
    "- üß† Use your own model architectures or training pipelines that you‚Äôre more comfortable with \n",
    "- ‚öóÔ∏è Experiment with ideas  \n",
    "- ü•á Compete creatively to climb the Kaggle leaderboard  \n",
    "- üôå Most importantly: **have fun** and **learn as much as you can** along the way\n",
    "\n",
    "This challenge simulates a real-world scientific problem, and there‚Äôs no single \"correct\" approach ‚Äî so be curious, experiment boldly, and make it your own!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available variables:\n",
      "- BC: (14114304, 5)\n",
      "- CH4: (4084, 3)\n",
      "- CO2: (4084, 3)\n",
      "- SO2: (14114304, 5)\n",
      "- pr: (42342912, 10)\n",
      "- rsdt: (14114304, 9)\n",
      "- tas: (42342912, 10)\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "# Load the zarr dataset\n",
    "ds = xr.open_zarr(\"processed_data_cse151b_v2_corrupted_ssp245.zarr\")\n",
    "\n",
    "# Convert to pandas dataframes - one for each variable\n",
    "dfs = {}\n",
    "for var in ds.data_vars:\n",
    "    # Reshape the 4D array (time, level, lat, lon) into a 2D dataframe\n",
    "    # Flatten spatial dimensions into single columns\n",
    "    df = ds[var].to_dataframe().reset_index()\n",
    "    \n",
    "    # Add meaningful column names for spatial coordinates\n",
    "    # First check if coordinates exist before assigning\n",
    "    if 'y' in df.columns:\n",
    "        df['latitude'] = df['y']\n",
    "    if 'x' in df.columns:  \n",
    "        df['longitude'] = df['x']\n",
    "    \n",
    "    # Store in dictionary\n",
    "    dfs[var] = df\n",
    "\n",
    "# Example usage:\n",
    "# Access individual dataframes like:\n",
    "# dfs['tas'] for temperature\n",
    "# dfs['pr'] for precipitation\n",
    "# etc.\n",
    "\n",
    "# Print available variables\n",
    "print(\"Available variables:\")\n",
    "for var in dfs.keys():\n",
    "    print(f\"- {var}: {dfs[var].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['BC', 'CH4', 'CO2', 'SO2', 'pr', 'rsdt', 'tas'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1021"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs['BC'][dfs['BC']['ssp'] == 'ssp126']['time'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ssp</th>\n",
       "      <th>time</th>\n",
       "      <th>member_id</th>\n",
       "      <th>y</th>\n",
       "      <th>x</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>pr</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-01-15 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>1.875</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>91.875</td>\n",
       "      <td>0.050370</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>1.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-01-15 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>6.875</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>6.875</td>\n",
       "      <td>0.047009</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>6.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-01-15 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>11.875</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>11.875</td>\n",
       "      <td>0.037265</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>11.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-01-15 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>16.875</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>16.875</td>\n",
       "      <td>0.029432</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>16.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-01-15 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>21.875</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>21.875</td>\n",
       "      <td>0.024712</td>\n",
       "      <td>-88.586387</td>\n",
       "      <td>21.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42342907</th>\n",
       "      <td>ssp585</td>\n",
       "      <td>2100-01-15 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>336.875</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>336.875</td>\n",
       "      <td>1.055148</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>336.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42342908</th>\n",
       "      <td>ssp585</td>\n",
       "      <td>2100-01-15 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>341.875</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>341.875</td>\n",
       "      <td>1.058902</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>341.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42342909</th>\n",
       "      <td>ssp585</td>\n",
       "      <td>2100-01-15 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>346.875</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>346.875</td>\n",
       "      <td>1.067653</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>346.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42342910</th>\n",
       "      <td>ssp585</td>\n",
       "      <td>2100-01-15 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>351.875</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>351.875</td>\n",
       "      <td>1.093096</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>351.875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42342911</th>\n",
       "      <td>ssp585</td>\n",
       "      <td>2100-01-15 00:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>356.875</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>356.875</td>\n",
       "      <td>1.125632</td>\n",
       "      <td>88.586387</td>\n",
       "      <td>356.875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42342912 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             ssp                 time  member_id          y        x  \\\n",
       "0         ssp126  2015-01-15 00:00:00          0 -88.586387    1.875   \n",
       "1         ssp126  2015-01-15 00:00:00          0 -88.586387    6.875   \n",
       "2         ssp126  2015-01-15 00:00:00          0 -88.586387   11.875   \n",
       "3         ssp126  2015-01-15 00:00:00          0 -88.586387   16.875   \n",
       "4         ssp126  2015-01-15 00:00:00          0 -88.586387   21.875   \n",
       "...          ...                  ...        ...        ...      ...   \n",
       "42342907  ssp585  2100-01-15 00:00:00          2  88.586387  336.875   \n",
       "42342908  ssp585  2100-01-15 00:00:00          2  88.586387  341.875   \n",
       "42342909  ssp585  2100-01-15 00:00:00          2  88.586387  346.875   \n",
       "42342910  ssp585  2100-01-15 00:00:00          2  88.586387  351.875   \n",
       "42342911  ssp585  2100-01-15 00:00:00          2  88.586387  356.875   \n",
       "\n",
       "                lat      lon        pr   latitude  longitude  \n",
       "0        -88.586387   91.875  0.050370 -88.586387      1.875  \n",
       "1        -88.586387    6.875  0.047009 -88.586387      6.875  \n",
       "2        -88.586387   11.875  0.037265 -88.586387     11.875  \n",
       "3        -88.586387   16.875  0.029432 -88.586387     16.875  \n",
       "4        -88.586387   21.875  0.024712 -88.586387     21.875  \n",
       "...             ...      ...       ...        ...        ...  \n",
       "42342907  88.586387  336.875  1.055148  88.586387    336.875  \n",
       "42342908  88.586387  341.875  1.058902  88.586387    341.875  \n",
       "42342909  88.586387  346.875  1.067653  88.586387    346.875  \n",
       "42342910  88.586387  351.875  1.093096  88.586387    351.875  \n",
       "42342911  88.586387  356.875  1.125632  88.586387    356.875  \n",
       "\n",
       "[42342912 rows x 10 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['pr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ssp</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>BC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10000</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>70.105263</td>\n",
       "      <td>321.25</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10001</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>70.105263</td>\n",
       "      <td>326.25</td>\n",
       "      <td>3.066604e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10002</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>70.105263</td>\n",
       "      <td>331.25</td>\n",
       "      <td>3.344667e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>70.105263</td>\n",
       "      <td>336.25</td>\n",
       "      <td>3.575927e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10004</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>70.105263</td>\n",
       "      <td>341.25</td>\n",
       "      <td>5.535903e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>70.105263</td>\n",
       "      <td>346.25</td>\n",
       "      <td>6.831938e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10006</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>70.105263</td>\n",
       "      <td>351.25</td>\n",
       "      <td>2.223533e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10007</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>70.105263</td>\n",
       "      <td>356.25</td>\n",
       "      <td>2.927953e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>1.25</td>\n",
       "      <td>5.750513e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>6.25</td>\n",
       "      <td>1.958630e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>11.25</td>\n",
       "      <td>3.362798e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>16.25</td>\n",
       "      <td>2.433683e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>21.25</td>\n",
       "      <td>9.383260e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10013</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>26.25</td>\n",
       "      <td>8.785151e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10014</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>31.25</td>\n",
       "      <td>1.348746e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10015</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>36.25</td>\n",
       "      <td>5.796838e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10016</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>41.25</td>\n",
       "      <td>1.465868e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10017</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>46.25</td>\n",
       "      <td>2.800880e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10018</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>51.25</td>\n",
       "      <td>1.491186e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10019</th>\n",
       "      <td>ssp126</td>\n",
       "      <td>2015-03-15 00:00:00</td>\n",
       "      <td>73.894737</td>\n",
       "      <td>56.25</td>\n",
       "      <td>8.870639e-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ssp                 time   latitude  longitude            BC\n",
       "10000  ssp126  2015-03-15 00:00:00  70.105263     321.25  0.000000e+00\n",
       "10001  ssp126  2015-03-15 00:00:00  70.105263     326.25  3.066604e-18\n",
       "10002  ssp126  2015-03-15 00:00:00  70.105263     331.25  3.344667e-16\n",
       "10003  ssp126  2015-03-15 00:00:00  70.105263     336.25  3.575927e-15\n",
       "10004  ssp126  2015-03-15 00:00:00  70.105263     341.25  5.535903e-15\n",
       "10005  ssp126  2015-03-15 00:00:00  70.105263     346.25  6.831938e-15\n",
       "10006  ssp126  2015-03-15 00:00:00  70.105263     351.25  2.223533e-15\n",
       "10007  ssp126  2015-03-15 00:00:00  70.105263     356.25  2.927953e-15\n",
       "10008  ssp126  2015-03-15 00:00:00  73.894737       1.25  5.750513e-15\n",
       "10009  ssp126  2015-03-15 00:00:00  73.894737       6.25  1.958630e-14\n",
       "10010  ssp126  2015-03-15 00:00:00  73.894737      11.25  3.362798e-14\n",
       "10011  ssp126  2015-03-15 00:00:00  73.894737      16.25  2.433683e-13\n",
       "10012  ssp126  2015-03-15 00:00:00  73.894737      21.25  9.383260e-14\n",
       "10013  ssp126  2015-03-15 00:00:00  73.894737      26.25  8.785151e-14\n",
       "10014  ssp126  2015-03-15 00:00:00  73.894737      31.25  1.348746e-13\n",
       "10015  ssp126  2015-03-15 00:00:00  73.894737      36.25  5.796838e-14\n",
       "10016  ssp126  2015-03-15 00:00:00  73.894737      41.25  1.465868e-14\n",
       "10017  ssp126  2015-03-15 00:00:00  73.894737      46.25  2.800880e-14\n",
       "10018  ssp126  2015-03-15 00:00:00  73.894737      51.25  1.491186e-14\n",
       "10019  ssp126  2015-03-15 00:00:00  73.894737      56.25  8.870639e-15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['BC'][dfs['BC']['ssp'] == 'ssp126'][10000:10020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs['BC']['longitude'].value_counts().index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ssp\n",
       "ssp126    1021\n",
       "ssp245    1021\n",
       "ssp370    1021\n",
       "ssp585    1021\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['CH4']['ssp'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3063"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2943 + 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3063"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1021*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class LossAnalyzer:\n",
    "    def __init__(self, datamodule, model, criterion=None):\n",
    "        \"\"\"\n",
    "        Initialize the loss analyzer with a datamodule and model.\n",
    "        \n",
    "        Args:\n",
    "            datamodule: The ClimateDataModule containing the data\n",
    "            model: The trained PyTorch model\n",
    "            criterion: Loss function (defaults to MSE if None)\n",
    "        \"\"\"\n",
    "        self.datamodule = datamodule\n",
    "        self.model = model\n",
    "        self.criterion = criterion or torch.nn.MSELoss(reduction='none')\n",
    "        self.device = next(model.parameters()).device\n",
    "        self.normalizer = datamodule.normalizer\n",
    "        \n",
    "        # Get the full training dataset\n",
    "        self.train_dataset = datamodule.train_dataset\n",
    "        self.lat, self.lon = datamodule.get_coords()\n",
    "        self.area_weights = datamodule.get_lat_weights()\n",
    "        \n",
    "        # Store variable names\n",
    "        self.input_vars = datamodule.input_vars\n",
    "        self.output_vars = datamodule.output_vars\n",
    "        \n",
    "    def compute_sample_losses(self, indices=None, batch_size=32):\n",
    "        \"\"\"\n",
    "        Compute per-sample losses for the specified indices or the entire dataset.\n",
    "        \n",
    "        Args:\n",
    "            indices: Optional list of indices to compute losses for\n",
    "            batch_size: Batch size for inference\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with sample losses and metadata\n",
    "        \"\"\"\n",
    "        if indices is None:\n",
    "            dataset = self.train_dataset\n",
    "        else:\n",
    "            dataset = Subset(self.train_dataset, indices)\n",
    "            \n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        all_losses = []\n",
    "        all_inputs = []\n",
    "        all_outputs = []\n",
    "        all_predictions = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in tqdm(dataloader, desc=\"Computing losses\"):\n",
    "                inputs = inputs.to(self.device)\n",
    "                targets = targets.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                predictions = self.model(inputs)\n",
    "                \n",
    "                # Compute loss per sample and per output variable\n",
    "                losses = self.criterion(predictions, targets)  # Shape: [batch, channels, height, width]\n",
    "                \n",
    "                # Compute spatial mean loss per sample and channel\n",
    "                sample_losses = losses.mean(dim=(2, 3))  # Shape: [batch, channels]\n",
    "                \n",
    "                # Store results\n",
    "                all_losses.append(sample_losses.cpu().numpy())\n",
    "                all_inputs.append(inputs.cpu().numpy())\n",
    "                all_outputs.append(targets.cpu().numpy())\n",
    "                all_predictions.append(predictions.cpu().numpy())\n",
    "        \n",
    "        # Concatenate results\n",
    "        all_losses = np.concatenate(all_losses, axis=0)\n",
    "        all_inputs = np.concatenate(all_inputs, axis=0)\n",
    "        all_outputs = np.concatenate(all_outputs, axis=0)\n",
    "        all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "        \n",
    "        # Create result dictionary\n",
    "        result = {\n",
    "            'losses': all_losses,  # Shape: [n_samples, n_output_vars]\n",
    "            'total_loss': all_losses.mean(axis=1),  # Mean loss across output variables\n",
    "            'inputs': all_inputs,  # Shape: [n_samples, n_input_vars, height, width]\n",
    "            'outputs': all_outputs,  # Shape: [n_samples, n_output_vars, height, width]\n",
    "            'predictions': all_predictions,  # Shape: [n_samples, n_output_vars, height, width]\n",
    "            'indices': indices if indices is not None else np.arange(len(dataset))\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def sample_by_criteria(self, criteria, n_samples=100):\n",
    "        \"\"\"\n",
    "        Sample indices based on different criteria.\n",
    "        \n",
    "        Args:\n",
    "            criteria: String indicating sampling method ('random', 'stratified', 'time_based', etc.)\n",
    "            n_samples: Number of samples to select\n",
    "            \n",
    "        Returns:\n",
    "            List of selected indices\n",
    "        \"\"\"\n",
    "        n_total = len(self.train_dataset)\n",
    "        \n",
    "        if criteria == 'random':\n",
    "            # Random sampling\n",
    "            return np.random.choice(n_total, size=n_samples, replace=False)\n",
    "        \n",
    "        elif criteria == 'stratified_by_ssp':\n",
    "            # Get SSP information for each sample\n",
    "            # This requires accessing the original data\n",
    "            # You'll need to modify this based on how your data is structured\n",
    "            ssps = []\n",
    "            for i in range(n_total):\n",
    "                # Get SSP for sample i\n",
    "                # This is a placeholder - you need to implement this\n",
    "                ssp = \"unknown\"  # Replace with actual code to get SSP\n",
    "                ssps.append(ssp)\n",
    "            \n",
    "            # Stratify by SSP\n",
    "            unique_ssps = np.unique(ssps)\n",
    "            samples_per_ssp = n_samples // len(unique_ssps)\n",
    "            \n",
    "            indices = []\n",
    "            for ssp in unique_ssps:\n",
    "                ssp_indices = np.where(np.array(ssps) == ssp)[0]\n",
    "                selected = np.random.choice(ssp_indices, size=samples_per_ssp, replace=False)\n",
    "                indices.extend(selected)\n",
    "            \n",
    "            return indices\n",
    "        \n",
    "        elif criteria == 'time_based':\n",
    "            # Sample evenly across time\n",
    "            # This assumes samples are ordered by time\n",
    "            step = n_total // n_samples\n",
    "            return np.arange(0, n_total, step)[:n_samples]\n",
    "        \n",
    "        elif criteria == 'spatial_regions':\n",
    "            # Sample different spatial regions\n",
    "            # This is a placeholder - implement based on your needs\n",
    "            regions = [\n",
    "                {'lat_range': (-90, -30), 'lon_range': (0, 360)},  # Southern high latitudes\n",
    "                {'lat_range': (-30, 30), 'lon_range': (0, 360)},   # Tropics\n",
    "                {'lat_range': (30, 90), 'lon_range': (0, 360)}     # Northern high latitudes\n",
    "            ]\n",
    "            \n",
    "            # You'll need to implement logic to select samples from these regions\n",
    "            # This is just a placeholder\n",
    "            return np.random.choice(n_total, size=n_samples, replace=False)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling criteria: {criteria}\")\n",
    "    \n",
    "    def analyze_high_loss_samples(self, result, top_n=20, variable_idx=None):\n",
    "        \"\"\"\n",
    "        Analyze the samples with highest loss.\n",
    "        \n",
    "        Args:\n",
    "            result: Result dictionary from compute_sample_losses\n",
    "            top_n: Number of highest-loss samples to analyze\n",
    "            variable_idx: Index of output variable to focus on (None for total loss)\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with analysis results\n",
    "        \"\"\"\n",
    "        if variable_idx is None:\n",
    "            # Use total loss across all variables\n",
    "            losses = result['total_loss']\n",
    "            var_name = \"total\"\n",
    "        else:\n",
    "            # Use loss for specific variable\n",
    "            losses = result['losses'][:, variable_idx]\n",
    "            var_name = self.output_vars[variable_idx]\n",
    "        \n",
    "        # Get indices of highest-loss samples\n",
    "        top_indices = np.argsort(losses)[-top_n:][::-1]\n",
    "        \n",
    "        # Get original dataset indices\n",
    "        original_indices = result['indices'][top_indices]\n",
    "        \n",
    "        # Get corresponding losses, inputs, outputs, and predictions\n",
    "        top_losses = losses[top_indices]\n",
    "        top_inputs = result['inputs'][top_indices]\n",
    "        top_outputs = result['outputs'][top_indices]\n",
    "        top_predictions = result['predictions'][top_indices]\n",
    "        \n",
    "        # Create analysis result\n",
    "        analysis = {\n",
    "            'variable': var_name,\n",
    "            'indices': original_indices,\n",
    "            'losses': top_losses,\n",
    "            'inputs': top_inputs,\n",
    "            'outputs': top_outputs,\n",
    "            'predictions': top_predictions\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def visualize_high_loss_samples(self, analysis, n_samples=5):\n",
    "        \"\"\"\n",
    "        Visualize high-loss samples.\n",
    "        \n",
    "        Args:\n",
    "            analysis: Analysis result from analyze_high_loss_samples\n",
    "            n_samples: Number of samples to visualize\n",
    "        \"\"\"\n",
    "        n_samples = min(n_samples, len(analysis['indices']))\n",
    "        var_name = analysis['variable']\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            idx = i\n",
    "            loss = analysis['losses'][idx]\n",
    "            \n",
    "            # Get data for visualization\n",
    "            if var_name == \"total\":\n",
    "                # For total loss, visualize all output variables\n",
    "                for v_idx, v_name in enumerate(self.output_vars):\n",
    "                    output = analysis['outputs'][idx, v_idx]\n",
    "                    prediction = analysis['predictions'][idx, v_idx]\n",
    "                    \n",
    "                    # Create xarray DataArrays for plotting\n",
    "                    output_xr = xr.DataArray(output, dims=[\"y\", \"x\"], \n",
    "                                            coords={\"y\": self.lat, \"x\": self.lon})\n",
    "                    pred_xr = xr.DataArray(prediction, dims=[\"y\", \"x\"], \n",
    "                                          coords={\"y\": self.lat, \"x\": self.lon})\n",
    "                    \n",
    "                    # Denormalize if needed\n",
    "                    if hasattr(self.normalizer, 'inverse_transform_output'):\n",
    "                        output_denorm = self.normalizer.inverse_transform_output(\n",
    "                            output.reshape(1, 1, *output.shape))[0, 0]\n",
    "                        pred_denorm = self.normalizer.inverse_transform_output(\n",
    "                            prediction.reshape(1, 1, *prediction.shape))[0, 0]\n",
    "                        \n",
    "                        output_xr = xr.DataArray(output_denorm, dims=[\"y\", \"x\"], \n",
    "                                               coords={\"y\": self.lat, \"x\": self.lon})\n",
    "                        pred_xr = xr.DataArray(pred_denorm, dims=[\"y\", \"x\"], \n",
    "                                             coords={\"y\": self.lat, \"x\": self.lon})\n",
    "                    \n",
    "                    # Plot\n",
    "                    fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "                    \n",
    "                    # Ground truth\n",
    "                    output_xr.plot(ax=axs[0], cmap='viridis')\n",
    "                    axs[0].set_title(f\"{v_name} - Ground Truth\")\n",
    "                    \n",
    "                    # Prediction\n",
    "                    pred_xr.plot(ax=axs[1], cmap='viridis')\n",
    "                    axs[1].set_title(f\"{v_name} - Prediction\")\n",
    "                    \n",
    "                    # Difference\n",
    "                    diff = pred_xr - output_xr\n",
    "                    diff.plot(ax=axs[2], cmap='RdBu_r')\n",
    "                    axs[2].set_title(f\"{v_name} - Difference\")\n",
    "                    \n",
    "                    plt.suptitle(f\"Sample {analysis['indices'][idx]}, Loss: {loss:.4f}\")\n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "            else:\n",
    "                # For specific variable\n",
    "                v_idx = self.output_vars.index(var_name)\n",
    "                output = analysis['outputs'][idx, v_idx]\n",
    "                prediction = analysis['predictions'][idx, v_idx]\n",
    "                \n",
    "                # Create xarray DataArrays for plotting\n",
    "                output_xr = xr.DataArray(output, dims=[\"y\", \"x\"], \n",
    "                                        coords={\"y\": self.lat, \"x\": self.lon})\n",
    "                pred_xr = xr.DataArray(prediction, dims=[\"y\", \"x\"], \n",
    "                                      coords={\"y\": self.lat, \"x\": self.lon})\n",
    "                \n",
    "                # Denormalize if needed\n",
    "                if hasattr(self.normalizer, 'inverse_transform_output'):\n",
    "                    output_denorm = self.normalizer.inverse_transform_output(\n",
    "                        output.reshape(1, 1, *output.shape))[0, 0]\n",
    "                    pred_denorm = self.normalizer.inverse_transform_output(\n",
    "                        prediction.reshape(1, 1, *prediction.shape))[0, 0]\n",
    "                    \n",
    "                    output_xr = xr.DataArray(output_denorm, dims=[\"y\", \"x\"], \n",
    "                                           coords={\"y\": self.lat, \"x\": self.lon})\n",
    "                    pred_xr = xr.DataArray(pred_denorm, dims=[\"y\", \"x\"], \n",
    "                                         coords={\"y\": self.lat, \"x\": self.lon})\n",
    "                \n",
    "                # Plot\n",
    "                fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "                \n",
    "                # Ground truth\n",
    "                output_xr.plot(ax=axs[0], cmap='viridis')\n",
    "                axs[0].set_title(f\"{var_name} - Ground Truth\")\n",
    "                \n",
    "                # Prediction\n",
    "                pred_xr.plot(ax=axs[1], cmap='viridis')\n",
    "                axs[1].set_title(f\"{var_name} - Prediction\")\n",
    "                \n",
    "                # Difference\n",
    "                diff = pred_xr - output_xr\n",
    "                diff.plot(ax=axs[2], cmap='RdBu_r')\n",
    "                axs[2].set_title(f\"{var_name} - Difference\")\n",
    "                \n",
    "                plt.suptitle(f\"Sample {analysis['indices'][idx]}, Loss: {loss:.4f}\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "    \n",
    "    def cluster_by_loss_patterns(self, result, n_clusters=5):\n",
    "        \"\"\"\n",
    "        Cluster samples based on their loss patterns across space.\n",
    "        \n",
    "        Args:\n",
    "            result: Result dictionary from compute_sample_losses\n",
    "            n_clusters: Number of clusters to create\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with clustering results\n",
    "        \"\"\"\n",
    "        # Compute spatial loss patterns\n",
    "        predictions = result['predictions']\n",
    "        outputs = result['outputs']\n",
    "        \n",
    "        # Compute loss per pixel\n",
    "        pixel_losses = (predictions - outputs)**2\n",
    "        \n",
    "        # Reshape to [n_samples, n_pixels * n_vars]\n",
    "        n_samples, n_vars, height, width = pixel_losses.shape\n",
    "        reshaped_losses = pixel_losses.reshape(n_samples, -1)\n",
    "        \n",
    "        # Apply PCA for dimensionality reduction\n",
    "        pca = PCA(n_components=min(50, reshaped_losses.shape[1]))\n",
    "        reduced_losses = pca.fit_transform(reshaped_losses)\n",
    "        \n",
    "        # Cluster the loss patterns\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        clusters = kmeans.fit_predict(reduced_losses)\n",
    "        \n",
    "        # Compute average loss per cluster\n",
    "        cluster_losses = []\n",
    "        for i in range(n_clusters):\n",
    "            mask = clusters == i\n",
    "            avg_loss = result['total_loss'][mask].mean()\n",
    "            cluster_losses.append(avg_loss)\n",
    "        \n",
    "        # Create clustering result\n",
    "        clustering = {\n",
    "            'clusters': clusters,\n",
    "            'cluster_losses': cluster_losses,\n",
    "            'indices': result['indices'],\n",
    "            'reduced_losses': reduced_losses\n",
    "        }\n",
    "        \n",
    "        return clustering\n",
    "    \n",
    "    def visualize_clusters(self, clustering):\n",
    "        \"\"\"\n",
    "        Visualize the clustering results.\n",
    "        \n",
    "        Args:\n",
    "            clustering: Clustering result from cluster_by_loss_patterns\n",
    "        \"\"\"\n",
    "        clusters = clustering['clusters']\n",
    "        reduced_losses = clustering['reduced_losses']\n",
    "        cluster_losses = clustering['cluster_losses']\n",
    "        \n",
    "        # Create a scatter plot of the first two PCA components\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(reduced_losses[:, 0], reduced_losses[:, 1], \n",
    "                             c=clusters, cmap='viridis', alpha=0.7)\n",
    "        \n",
    "        # Add cluster centers\n",
    "        for i in range(len(cluster_losses)):\n",
    "            mask = clusters == i\n",
    "            center_x = reduced_losses[mask, 0].mean()\n",
    "            center_y = reduced_losses[mask, 1].mean()\n",
    "            plt.annotate(f\"Cluster {i}\\nLoss: {cluster_losses[i]:.4f}\",\n",
    "                        (center_x, center_y),\n",
    "                        fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.title('Clustering of Samples by Loss Patterns')\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot average loss per cluster\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        bars = plt.bar(range(len(cluster_losses)), cluster_losses)\n",
    "        \n",
    "        # Add count of samples per cluster\n",
    "        for i in range(len(cluster_losses)):\n",
    "            count = np.sum(clusters == i)\n",
    "            plt.annotate(f\"n={count}\",\n",
    "                        (i, cluster_losses[i]),\n",
    "                        ha='center', va='bottom')\n",
    "        \n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Average Loss')\n",
    "        plt.title('Average Loss per Cluster')\n",
    "        plt.xticks(range(len(cluster_losses)))\n",
    "        plt.grid(True, axis='y', alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_loss_by_region(self, result):\n",
    "        \"\"\"\n",
    "        Analyze loss patterns by geographical region.\n",
    "        \n",
    "        Args:\n",
    "            result: Result dictionary from compute_sample_losses\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with regional analysis\n",
    "        \"\"\"\n",
    "        predictions = result['predictions']\n",
    "        outputs = result['outputs']\n",
    "        \n",
    "        # Compute loss per pixel\n",
    "        pixel_losses = (predictions - outputs)**2  # Shape: [n_samples, n_vars, height, width]\n",
    "        \n",
    "        # Define regions (latitude bands)\n",
    "        regions = [\n",
    "            {'name': 'Arctic', 'lat_range': (66.5, 90)},\n",
    "            {'name': 'Northern Mid-Latitudes', 'lat_range': (30, 66.5)},\n",
    "            {'name': 'Tropics', 'lat_range': (-30, 30)},\n",
    "            {'name': 'Southern Mid-Latitudes', 'lat_range': (-66.5, -30)},\n",
    "            {'name': 'Antarctic', 'lat_range': (-90, -66.5)}\n",
    "        ]\n",
    "        \n",
    "        # Compute average loss per region\n",
    "        region_losses = []\n",
    "        for region in regions:\n",
    "            lat_min, lat_max = region['lat_range']\n",
    "            \n",
    "            # Find latitude indices within the range\n",
    "            lat_indices = np.where((self.lat >= lat_min) & (self.lat <= lat_max))[0]\n",
    "            \n",
    "            if len(lat_indices) == 0:\n",
    "                continue\n",
    "                \n",
    "            # Compute average loss in this region\n",
    "            region_loss = pixel_losses[:, :, lat_indices, :].mean(axis=(0, 2, 3))\n",
    "            \n",
    "            region_losses.append({\n",
    "                'name': region['name'],\n",
    "                'lat_range': region['lat_range'],\n",
    "                'loss': region_loss,\n",
    "                'total_loss': region_loss.mean()\n",
    "            })\n",
    "        \n",
    "        return {'regions': region_losses}\n",
    "    \n",
    "    def visualize_regional_analysis(self, regional_analysis):\n",
    "        \"\"\"\n",
    "        Visualize the regional loss analysis.\n",
    "        \n",
    "        Args:\n",
    "            regional_analysis: Result from analyze_loss_by_region\n",
    "        \"\"\"\n",
    "        regions = regional_analysis['regions']\n",
    "        \n",
    "        # Plot total loss by region\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        names = [r['name'] for r in regions]\n",
    "        losses = [r['total_loss'] for r in regions]\n",
    "        \n",
    "        bars = plt.bar(names, losses)\n",
    "        \n",
    "        plt.xlabel('Region')\n",
    "        plt.ylabel('Average Loss')\n",
    "        plt.title('Average Loss by Geographical Region')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, axis='y', alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot loss by variable and region\n",
    "        if len(self.output_vars) > 1:\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            \n",
    "            x = np.arange(len(regions))\n",
    "            width = 0.8 / len(self.output_vars)\n",
    "            \n",
    "            for i, var in enumerate(self.output_vars):\n",
    "                var_losses = [r['loss'][i] for r in regions]\n",
    "                offset = (i - len(self.output_vars)/2 + 0.5) * width\n",
    "                plt.bar(x + offset, var_losses, width, label=var)\n",
    "            \n",
    "            plt.xlabel('Region')\n",
    "            plt.ylabel('Average Loss')\n",
    "            plt.title('Average Loss by Variable and Region')\n",
    "            plt.xticks(x, names, rotation=45)\n",
    "            plt.legend()\n",
    "            plt.grid(True, axis='y', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Example usage\n",
    "def run_loss_analysis(datamodule, model, sample_size=500):\n",
    "    \"\"\"\n",
    "    Run a complete loss analysis workflow.\n",
    "    \n",
    "    Args:\n",
    "        datamodule: The ClimateDataModule\n",
    "        model: The trained model\n",
    "        sample_size: Number of samples to analyze\n",
    "    \"\"\"\n",
    "    analyzer = LossAnalyzer(datamodule, model)\n",
    "    \n",
    "    # Sample random subset\n",
    "    print(f\"Sampling {sample_size} random samples...\")\n",
    "    indices = analyzer.sample_by_criteria('random', n_samples=sample_size)\n",
    "    \n",
    "    # Compute losses\n",
    "    print(\"Computing losses...\")\n",
    "    result = analyzer.compute_sample_losses(indices)\n",
    "    \n",
    "    # Analyze high-loss samples\n",
    "    print(\"Analyzing high-loss samples...\")\n",
    "    analysis = analyzer.analyze_high_loss_samples(result, top_n=10)\n",
    "    \n",
    "    # Visualize high-loss samples\n",
    "    print(\"Visualizing high-loss samples...\")\n",
    "    analyzer.visualize_high_loss_samples(analysis, n_samples=3)\n",
    "    \n",
    "    # Cluster by loss patterns\n",
    "    print(\"Clustering samples by loss patterns...\")\n",
    "    clustering = analyzer.cluster_by_loss_patterns(result, n_clusters=5)\n",
    "    \n",
    "    # Visualize clusters\n",
    "    print(\"Visualizing clusters...\")\n",
    "    analyzer.visualize_clusters(clustering)\n",
    "    \n",
    "    # Analyze by region\n",
    "    print(\"Analyzing loss by region...\")\n",
    "    regional_analysis = analyzer.analyze_loss_by_region(result)\n",
    "    \n",
    "    # Visualize regional analysis\n",
    "    print(\"Visualizing regional analysis...\")\n",
    "    analyzer.visualize_regional_analysis(regional_analysis)\n",
    "    \n",
    "    return {\n",
    "        'result': result,\n",
    "        'analysis': analysis,\n",
    "        'clustering': clustering,\n",
    "        'regional_analysis': regional_analysis\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ClimateXGBoostPredictor:\n",
    "    \"\"\"\n",
    "    A class to predict climate variables (pr and tas) using XGBoost regressors.\n",
    "    This approach flattens spatial dimensions and treats each grid cell as a separate prediction task.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_estimators=100, max_depth=6, learning_rate=0.1, subsample=0.8):\n",
    "        \"\"\"\n",
    "        Initialize the XGBoost predictor with hyperparameters.\n",
    "        \n",
    "        Args:\n",
    "            n_estimators: Number of boosting rounds\n",
    "            max_depth: Maximum tree depth\n",
    "            learning_rate: Step size shrinkage\n",
    "            subsample: Subsample ratio of training instances\n",
    "        \"\"\"\n",
    "        self.params = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'learning_rate': learning_rate,\n",
    "            'subsample': subsample,\n",
    "            'objective': 'reg:squarederror',\n",
    "            'tree_method': 'hist',  # For faster training\n",
    "            'verbosity': 1\n",
    "        }\n",
    "        \n",
    "        # Create separate models for each output variable\n",
    "        self.pr_model = xgb.XGBRegressor(**self.params)\n",
    "        self.tas_model = xgb.XGBRegressor(**self.params)\n",
    "        \n",
    "        # Scalers for input and output normalization\n",
    "        self.input_scaler = StandardScaler()\n",
    "        self.pr_scaler = StandardScaler()\n",
    "        self.tas_scaler = StandardScaler()\n",
    "        \n",
    "        # Store grid dimensions for reshaping predictions\n",
    "        self.grid_shape = None\n",
    "        self.input_vars = None\n",
    "        self.lat = None\n",
    "        self.lon = None\n",
    "    \n",
    "    def _prepare_data(self, ds, input_vars, output_vars=['pr', 'tas']):\n",
    "        \"\"\"\n",
    "        Prepare data from xarray Dataset for XGBoost training.\n",
    "        \n",
    "        Args:\n",
    "            ds: xarray Dataset containing climate data\n",
    "            input_vars: List of input variable names\n",
    "            output_vars: List of output variable names\n",
    "            \n",
    "        Returns:\n",
    "            X: Input features (time, variables, lat, lon) reshaped to (time, variables*lat*lon)\n",
    "            y_pr: Precipitation targets\n",
    "            y_tas: Temperature targets\n",
    "        \"\"\"\n",
    "        # Store input variables for prediction\n",
    "        self.input_vars = input_vars\n",
    "        \n",
    "        # Extract coordinates\n",
    "        self.lat = ds.lat.values\n",
    "        self.lon = ds.lon.values\n",
    "        \n",
    "        # Store grid shape for reshaping predictions\n",
    "        self.grid_shape = (len(self.lat), len(self.lon))\n",
    "        \n",
    "        # Extract input variables and stack them\n",
    "        X_list = []\n",
    "        for var in input_vars:\n",
    "            if var in ds:\n",
    "                # Handle different dimensions\n",
    "                if 'lat' in ds[var].dims and 'lon' in ds[var].dims:\n",
    "                    # Spatial variable\n",
    "                    X_var = ds[var].values\n",
    "                    # Reshape to (time, lat*lon)\n",
    "                    X_var = X_var.reshape(X_var.shape[0], -1)\n",
    "                else:\n",
    "                    # Non-spatial variable (e.g., global forcing)\n",
    "                    X_var = ds[var].values.reshape(-1, 1)\n",
    "                    # Repeat for each grid cell\n",
    "                    X_var = np.repeat(X_var, self.grid_shape[0] * self.grid_shape[1], axis=1)\n",
    "                \n",
    "                X_list.append(X_var)\n",
    "        \n",
    "        # Concatenate all input variables\n",
    "        X = np.concatenate(X_list, axis=1)\n",
    "        \n",
    "        # Extract output variables\n",
    "        y_pr = ds['pr'].values.reshape(ds['pr'].shape[0], -1)\n",
    "        y_tas = ds['tas'].values.reshape(ds['tas'].shape[0], -1)\n",
    "        \n",
    "        return X, y_pr, y_tas\n",
    "    \n",
    "    def fit(self, ds, input_vars, output_vars=['pr', 'tas'], test_size=0.2):\n",
    "        \"\"\"\n",
    "        Fit XGBoost models to predict climate variables.\n",
    "        \n",
    "        Args:\n",
    "            ds: xarray Dataset containing climate data\n",
    "            input_vars: List of input variable names\n",
    "            output_vars: List of output variable names\n",
    "            test_size: Fraction of data to use for validation\n",
    "            \n",
    "        Returns:\n",
    "            self: The fitted predictor\n",
    "        \"\"\"\n",
    "        print(\"Preparing data...\")\n",
    "        X, y_pr, y_tas = self._prepare_data(ds, input_vars, output_vars)\n",
    "        \n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val, y_pr_train, y_pr_val, y_tas_train, y_tas_val = train_test_split(\n",
    "            X, y_pr, y_tas, test_size=test_size, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Normalize inputs and outputs\n",
    "        print(\"Normalizing data...\")\n",
    "        X_train = self.input_scaler.fit_transform(X_train)\n",
    "        X_val = self.input_scaler.transform(X_val)\n",
    "        \n",
    "        y_pr_train = self.pr_scaler.fit_transform(y_pr_train)\n",
    "        y_pr_val = self.pr_scaler.transform(y_pr_val)\n",
    "        \n",
    "        y_tas_train = self.tas_scaler.fit_transform(y_tas_train)\n",
    "        y_tas_val = self.tas_scaler.transform(y_tas_val)\n",
    "        \n",
    "        # Train precipitation model\n",
    "        print(\"Training precipitation model...\")\n",
    "        self.pr_model.fit(\n",
    "            X_train, y_pr_train,\n",
    "            eval_set=[(X_val, y_pr_val)],\n",
    "            eval_metric='rmse',\n",
    "            early_stopping_rounds=10,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Train temperature model\n",
    "        print(\"Training temperature model...\")\n",
    "        self.tas_model.fit(\n",
    "            X_train, y_tas_train,\n",
    "            eval_set=[(X_val, y_tas_val)],\n",
    "            eval_metric='rmse',\n",
    "            early_stopping_rounds=10,\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        pr_pred_val = self.pr_model.predict(X_val)\n",
    "        tas_pred_val = self.tas_model.predict(X_val)\n",
    "        \n",
    "        # Denormalize predictions and targets\n",
    "        pr_pred_val = self.pr_scaler.inverse_transform(pr_pred_val)\n",
    "        pr_val = self.pr_scaler.inverse_transform(y_pr_val)\n",
    "        \n",
    "        tas_pred_val = self.tas_scaler.inverse_transform(tas_pred_val)\n",
    "        tas_val = self.tas_scaler.inverse_transform(y_tas_val)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        pr_rmse = np.sqrt(mean_squared_error(pr_val, pr_pred_val))\n",
    "        tas_rmse = np.sqrt(mean_squared_error(tas_val, tas_pred_val))\n",
    "        \n",
    "        print(f\"Validation RMSE - Precipitation: {pr_rmse:.4f}, Temperature: {tas_rmse:.4f}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, ds_test):\n",
    "        \"\"\"\n",
    "        Make predictions on new data.\n",
    "        \n",
    "        Args:\n",
    "            ds_test: xarray Dataset containing test data\n",
    "            \n",
    "        Returns:\n",
    "            ds_pred: xarray Dataset with predictions\n",
    "        \"\"\"\n",
    "        # Prepare test data\n",
    "        X_test, _, _ = self._prepare_data(ds_test, self.input_vars)\n",
    "        \n",
    "        # Normalize inputs\n",
    "        X_test = self.input_scaler.transform(X_test)\n",
    "        \n",
    "        # Make predictions\n",
    "        pr_pred = self.pr_model.predict(X_test)\n",
    "        tas_pred = self.tas_model.predict(X_test)\n",
    "        \n",
    "        # Denormalize predictions\n",
    "        pr_pred = self.pr_scaler.inverse_transform(pr_pred)\n",
    "        tas_pred = self.tas_scaler.inverse_transform(tas_pred)\n",
    "        \n",
    "        # Reshape predictions to original grid shape\n",
    "        pr_pred = pr_pred.reshape(-1, self.grid_shape[0], self.grid_shape[1])\n",
    "        tas_pred = tas_pred.reshape(-1, self.grid_shape[0], self.grid_shape[1])\n",
    "        \n",
    "        # Create output dataset\n",
    "        ds_pred = xr.Dataset(\n",
    "            data_vars={\n",
    "                'pr': (['time', 'lat', 'lon'], pr_pred),\n",
    "                'tas': (['time', 'lat', 'lon'], tas_pred)\n",
    "            },\n",
    "            coords={\n",
    "                'time': ds_test.time.values,\n",
    "                'lat': self.lat,\n",
    "                'lon': self.lon\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return ds_pred\n",
    "    \n",
    "    def save_submission(self, ds_pred, filename='submission.csv'):\n",
    "        \"\"\"\n",
    "        Save predictions to a CSV file in the required format for submission.\n",
    "        \n",
    "        Args:\n",
    "            ds_pred: xarray Dataset with predictions\n",
    "            filename: Output CSV filename\n",
    "        \"\"\"\n",
    "        # Create empty list to store rows\n",
    "        rows = []\n",
    "        \n",
    "        # Get variables and their units\n",
    "        var_units = {'pr': 'mm/day', 'tas': 'K'}\n",
    "        \n",
    "        # Loop through variables, times, lats, lons\n",
    "        for var in ['pr', 'tas']:\n",
    "            for t_idx, time in enumerate(ds_pred.time.values):\n",
    "                for lat_idx, lat in enumerate(ds_pred.lat.values):\n",
    "                    for lon_idx, lon in enumerate(ds_pred.lon.values):\n",
    "                        # Create ID string\n",
    "                        id_str = f\"{var}_{time.strftime('%Y-%m-%d')}_{lat:.2f}_{lon:.2f}\"\n",
    "                        \n",
    "                        # Get prediction value\n",
    "                        value = float(ds_pred[var].values[t_idx, lat_idx, lon_idx])\n",
    "                        \n",
    "                        # Add row\n",
    "                        rows.append({'ID': id_str, 'Predicted': value})\n",
    "        \n",
    "        # Create DataFrame and save to CSV\n",
    "        df = pd.DataFrame(rows)\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Submission saved to {filename}\")\n",
    "    \n",
    "    def plot_predictions(self, ds_true, ds_pred, time_idx=0, cmap='viridis'):\n",
    "        \"\"\"\n",
    "        Plot true values vs predictions for visual comparison.\n",
    "        \n",
    "        Args:\n",
    "            ds_true: xarray Dataset with true values\n",
    "            ds_pred: xarray Dataset with predictions\n",
    "            time_idx: Time index to plot\n",
    "            cmap: Colormap to use\n",
    "        \"\"\"\n",
    "        fig, axs = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # Plot precipitation\n",
    "        vmin = min(ds_true.pr.values[time_idx].min(), ds_pred.pr.values[time_idx].min())\n",
    "        vmax = max(ds_true.pr.values[time_idx].max(), ds_pred.pr.values[time_idx].max())\n",
    "        \n",
    "        im0 = axs[0, 0].imshow(ds_true.pr.values[time_idx], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "        axs[0, 0].set_title('True Precipitation')\n",
    "        plt.colorbar(im0, ax=axs[0, 0])\n",
    "        \n",
    "        im1 = axs[0, 1].imshow(ds_pred.pr.values[time_idx], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "        axs[0, 1].set_title('Predicted Precipitation')\n",
    "        plt.colorbar(im1, ax=axs[0, 1])\n",
    "        \n",
    "        diff_pr = ds_pred.pr.values[time_idx] - ds_true.pr.values[time_idx]\n",
    "        im2 = axs[0, 2].imshow(diff_pr, cmap='RdBu_r')\n",
    "        axs[0, 2].set_title('Difference (Pred - True)')\n",
    "        plt.colorbar(im2, ax=axs[0, 2])\n",
    "        \n",
    "        # Plot temperature\n",
    "        vmin = min(ds_true.tas.values[time_idx].min(), ds_pred.tas.values[time_idx].min())\n",
    "        vmax = max(ds_true.tas.values[time_idx].max(), ds_pred.tas.values[time_idx].max())\n",
    "        \n",
    "        im3 = axs[1, 0].imshow(ds_true.tas.values[time_idx], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "        axs[1, 0].set_title('True Temperature')\n",
    "        plt.colorbar(im3, ax=axs[1, 0])\n",
    "        \n",
    "        im4 = axs[1, 1].imshow(ds_pred.tas.values[time_idx], cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "        axs[1, 1].set_title('Predicted Temperature')\n",
    "        plt.colorbar(im4, ax=axs[1, 1])\n",
    "        \n",
    "        diff_tas = ds_pred.tas.values[time_idx] - ds_true.tas.values[time_idx]\n",
    "        im5 = axs[1, 2].imshow(diff_tas, cmap='RdBu_r')\n",
    "        axs[1, 2].set_title('Difference (Pred - True)')\n",
    "        plt.colorbar(im5, ax=axs[1, 2])\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load data\n",
    "    ds = xr.open_zarr(\"path/to/climate_data.zarr\")\n",
    "    \n",
    "    # Define input variables\n",
    "    input_vars = ['CO2', 'CH4', 'N2O', 'SO2', 'BC', 'OC', 'NH3', 'NOx', 'VOC', 'ozone']\n",
    "    \n",
    "    # Create and train the model\n",
    "    predictor = ClimateXGBoostPredictor(\n",
    "        n_estimators=200,\n",
    "        max_depth=8,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8\n",
    "    )\n",
    "    \n",
    "    # Split data into train and test\n",
    "    train_years = slice('2015', '2090')\n",
    "    test_years = slice('2090', '2100')\n",
    "    \n",
    "    ds_train = ds.sel(time=train_years)\n",
    "    ds_test = ds.sel(time=test_years)\n",
    "    \n",
    "    # Train the model\n",
    "    predictor.fit(ds_train, input_vars)\n",
    "    \n",
    "    # Make predictions\n",
    "    ds_pred = predictor.predict(ds_test)\n",
    "    \n",
    "    # Plot some results\n",
    "    predictor.plot_predictions(ds_test, ds_pred, time_idx=0)\n",
    "    \n",
    "    # Save submission\n",
    "    predictor.save_submission(ds_pred, 'xgboost_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
