{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🌎 Welcome to the CSE151B Spring 2025 Climate Emulation Competition!\n",
    "\n",
    "Thank you for participating in this exciting challenge focused on building machine learning models to emulate complex climate systems.  \n",
    "This notebook is provided as a **starter template** to help you:\n",
    "\n",
    "- Understand how to load and preprocess the dataset  \n",
    "- Construct a baseline model  \n",
    "- Train and evaluate predictions using a PyTorch Lightning pipeline  \n",
    "- Format your predictions for submission to the leaderboard  \n",
    "\n",
    "You're encouraged to:\n",
    "- Build on this structure or replace it entirely\n",
    "- Try more advanced models and training strategies\n",
    "- Incorporate your own ideas to push the boundaries of what's possible\n",
    "\n",
    "If you're interested in developing within a repository structure and/or use helpful tools like configuration management (based on Hydra) and logging (with Weights & Biases), we recommend checking out the following Github repo. Such a structure can be useful when running multiple experiments and trying various research ideas.\n",
    "\n",
    "👉 [https://github.com/salvaRC/cse151b-spring2025-competition](https://github.com/salvaRC/cse151b-spring2025-competition)\n",
    "\n",
    "Good luck, have fun, and we hope you learn a lot through this process!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Install Required Libraries\n",
    "We install the necessary Python packages for data loading, deep learning, and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xarray zarr dask lightning matplotlib wandb cftime einops --quiet\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import lightning.pytorch as pl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ Configuration Setup  \n",
    "Define all model, data, and training hyperparameters in one place for easy control and reproducibility.\n",
    "\n",
    "### 📊 Data Configuration\n",
    "\n",
    "We define the dataset settings used for training and evaluation. This includes:\n",
    "\n",
    "- **`path`**: Path to the `.zarr` dataset containing monthly climate variables from CMIP6 simulations.\n",
    "- **`input_vars`**: Climate forcing variables (e.g., CO₂, CH₄) used as model inputs.\n",
    "- **`output_vars`**: Target variables to predict — surface air temperature (`tas`) and precipitation (`pr`).\n",
    "- **`target_member_id`**: Ensemble member to use from the simulations (each SSP has 3) for target variables.\n",
    "- **`train_ssps`**: SSP scenarios used for training (low to high emissions).\n",
    "- **`test_ssp`**: Scenario held out for evaluation (Must be set to SSP245).\n",
    "- **`test_months`**: Number of months to include in the test split (Must be set to 120).\n",
    "- **`batch_size`** and **`num_workers`**: Data loading parameters for PyTorch training.\n",
    "\n",
    "These settings reflect how the challenge is structured: models must learn from some emission scenarios and generalize to unseen ones.\n",
    "\n",
    "> ⚠️ **Important:** Do **not modify** the following test settings:\n",
    ">\n",
    "> - `test_ssp` must remain **`ssp245`**, which is the held-out evaluation scenario.\n",
    "> - `test_months` must be **`120`**, corresponding to the last 10 years (monthly resolution) of the scenario.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE Change the data directory according to where you have your zarr files stored\n",
    "config = {\n",
    "    \"data\": {\n",
    "        \"path\": \"/home/wcw003/private/processed_data_cse151b_v2_corrupted_ssp245.zarr\",\n",
    "        \"input_vars\": [\"CO2\", \"SO2\", \"CH4\", \"BC\", \"rsdt\"],\n",
    "        \"output_vars\": [\"tas\", \"pr\"],\n",
    "        \"target_member_id\": 0,\n",
    "        \"train_ssps\": [\"ssp126\", \"ssp370\", \"ssp585\"],\n",
    "        \"test_ssp\": \"ssp245\",\n",
    "        \"test_months\": 360,\n",
    "        \"batch_size\": 8,\n",
    "        \"num_workers\": 4,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"type\": \"simple_cnn\",\n",
    "        \"kernel_size\": 3,\n",
    "        \"init_dim\": 64,\n",
    "        \"depth\": 4,\n",
    "        \"dropout_rate\": 0.1,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"lr\": 1e-3,\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"max_epochs\": 30, # originally 10\n",
    "        \"accelerator\": \"auto\",\n",
    "        \"devices\": \"auto\",\n",
    "        \"precision\": 32,\n",
    "        \"deterministic\": True,\n",
    "        \"num_sanity_val_steps\": 0\n",
    "    },\n",
    "    \"seed\": 42,\n",
    "}\n",
    "pl.seed_everything(config[\"seed\"])  # Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔧 Spatial Weighting Utility Function\n",
    "\n",
    "This cell sets up utility functions for reproducibility and spatial weighting:\n",
    "\n",
    "- **`get_lat_weights(latitude_values)`**: Computes cosine-based area weights for each latitude, accounting for the Earth's curvature. This is critical for evaluating global climate metrics fairly — grid cells near the equator represent larger surface areas than those near the poles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_weights(latitude_values):\n",
    "    lat_rad = np.deg2rad(latitude_values)\n",
    "    weights = np.cos(lat_rad)\n",
    "    return weights / np.mean(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 SimpleCNN: A Residual Convolutional Baseline\n",
    "\n",
    "This is a lightweight baseline model designed to capture spatial patterns in global climate data using convolutional layers.\n",
    "\n",
    "- The architecture starts with a **convolution + batch norm + ReLU** block to process the input channels.\n",
    "- It then applies a series of **residual blocks** to extract increasingly abstract spatial features. These help preserve gradient flow during training.\n",
    "- Finally, a few convolutional layers reduce the feature maps down to the desired number of output channels (`tas` and `pr`).\n",
    "\n",
    "This model only serves as a **simple baseline for climate emulation**. \n",
    "\n",
    "We encourage you to build and experiment with your own models and ideas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first modify\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super().__init__()\n",
    "        # Increase kernel size for capturing larger spatial patterns in climate data\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=5, stride=stride, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # Use LeakyReLU for better gradient flow with climate data\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.act(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.skip(identity)\n",
    "        return self.act(out)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, n_input_channels, n_output_channels, kernel_size=5, init_dim=128, depth=6, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        # Increased initial dimensions and depth for climate modeling capacity\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, init_dim, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm2d(init_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        current_dim = init_dim\n",
    "        for i in range(depth):\n",
    "            out_dim = current_dim * 2 if i < depth - 1 else current_dim\n",
    "            self.res_blocks.append(ResidualBlock(current_dim, out_dim))\n",
    "            if i < depth - 1:\n",
    "                current_dim *= 2\n",
    "                \n",
    "        # Added spatial attention for focusing on important climate patterns\n",
    "        self.spatial_attention = nn.Sequential(\n",
    "            nn.Conv2d(current_dim, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        \n",
    "        # Modified final layers for better climate variable prediction\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(current_dim, current_dim // 2, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm2d(current_dim // 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(current_dim // 2, current_dim // 4, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(current_dim // 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(current_dim // 4, n_output_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "            \n",
    "        # Apply spatial attention\n",
    "        attention = self.spatial_attention(x)\n",
    "        x = x * attention\n",
    "        \n",
    "        return self.final(self.dropout(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    \"\"\"Applies a module over multiple time steps\"\"\"\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape to (batch_size * time_steps, ...)\n",
    "        batch_size, time_steps = x.size(0), x.size(1)\n",
    "        x_reshaped = x.contiguous().view(batch_size * time_steps, *x.size()[2:])\n",
    "        \n",
    "        # Apply module\n",
    "        y = self.module(x_reshaped)\n",
    "        \n",
    "        # Reshape back\n",
    "        return y.contiguous().view(batch_size, time_steps, *y.size()[1:])\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial attention mechanism for focusing on important regions\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate attention map\n",
    "        attention = torch.sigmoid(self.conv(x))\n",
    "        return x * attention\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"Temporal attention mechanism for focusing on important time steps\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time_steps, features]\n",
    "        # Generate attention weights\n",
    "        attention_weights = F.softmax(self.fc(x), dim=1)\n",
    "        # Apply attention\n",
    "        context = torch.sum(x * attention_weights, dim=1)\n",
    "        return context\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with larger kernels for climate patterns\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, \n",
    "                              stride=stride, padding=kernel_size//2, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n",
    "                              stride=1, padding=kernel_size//2, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection if dimensions change\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class TimeSeriesClimateCNN(nn.Module):\n",
    "    \"\"\"CNN model for climate time series prediction with optional LSTM\"\"\"\n",
    "    def __init__(self, n_input_channels, n_output_channels, seq_length=12, \n",
    "                 kernel_size=5, init_dim=64, depth=4, dropout_rate=0.3, use_lstm=True):\n",
    "        super(TimeSeriesClimateCNN, self).__init__()\n",
    "        \n",
    "        self.n_input_channels = n_input_channels\n",
    "        self.n_output_channels = n_output_channels\n",
    "        self.seq_length = seq_length\n",
    "        self.use_lstm = use_lstm\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        self.initial_conv = TimeDistributed(\n",
    "            nn.Conv2d(n_input_channels, init_dim, kernel_size=kernel_size, \n",
    "                     stride=1, padding=kernel_size//2)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks with increasing channels\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        channels = [init_dim * (2**i) for i in range(depth)]\n",
    "        \n",
    "        for i in range(depth-1):\n",
    "            block = TimeDistributed(\n",
    "                ResidualBlock(channels[i], channels[i+1], kernel_size=kernel_size, stride=2)\n",
    "            )\n",
    "            self.res_blocks.append(block)\n",
    "        \n",
    "        # Spatial attention after CNN layers\n",
    "        self.spatial_attention = TimeDistributed(SpatialAttention(channels[-1]))\n",
    "        \n",
    "        # Global average pooling to reduce spatial dimensions\n",
    "        self.gap = TimeDistributed(nn.AdaptiveAvgPool2d(1))\n",
    "        \n",
    "        # LSTM for temporal modeling\n",
    "        if use_lstm:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=channels[-1],\n",
    "                hidden_size=channels[-1],\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout_rate,\n",
    "                bidirectional=True\n",
    "            )\n",
    "            lstm_output_dim = channels[-1] * 2  # bidirectional\n",
    "        else:\n",
    "            lstm_output_dim = channels[-1]\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.temporal_attention = TemporalAttention(lstm_output_dim)\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Upsampling to original resolution\n",
    "        self.upsampling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(lstm_output_dim, channels[-2], kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(channels[-2], channels[-3], kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(channels[-3], n_output_channels, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time_steps, channels, height, width]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Apply initial convolution\n",
    "        x = self.initial_conv(x)\n",
    "        \n",
    "        # Apply residual blocks\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        x = self.spatial_attention(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.gap(x)\n",
    "        x = x.view(batch_size, self.seq_length, -1)  # [batch, time_steps, channels]\n",
    "        \n",
    "        # Apply LSTM if enabled\n",
    "        if self.use_lstm:\n",
    "            x, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply temporal attention to focus on important time steps\n",
    "        x = self.temporal_attention(x)  # [batch, channels]\n",
    "        \n",
    "        # Reshape for upsampling\n",
    "        x = x.view(batch_size, -1, 1, 1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Upsampling to get final prediction\n",
    "        x = self.upsampling(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ClimateTransformerCNN(nn.Module):\n",
    "    \"\"\"CNN model with transformer for temporal modeling\"\"\"\n",
    "    def __init__(self, n_input_channels, n_output_channels, seq_length=12, \n",
    "                 kernel_size=5, init_dim=64, depth=4, n_heads=8, dropout_rate=0.3):\n",
    "        super(ClimateTransformerCNN, self).__init__()\n",
    "        \n",
    "        self.n_input_channels = n_input_channels\n",
    "        self.n_output_channels = n_output_channels\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        self.initial_conv = TimeDistributed(\n",
    "            nn.Conv2d(n_input_channels, init_dim, kernel_size=kernel_size, \n",
    "                     stride=1, padding=kernel_size//2)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks with increasing channels\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        channels = [init_dim * (2**i) for i in range(depth)]\n",
    "        \n",
    "        for i in range(depth-1):\n",
    "            block = TimeDistributed(\n",
    "                ResidualBlock(channels[i], channels[i+1], kernel_size=kernel_size, stride=2)\n",
    "            )\n",
    "            self.res_blocks.append(block)\n",
    "        \n",
    "        # Spatial attention after CNN layers\n",
    "        self.spatial_attention = TimeDistributed(SpatialAttention(channels[-1]))\n",
    "        \n",
    "        # Global average pooling to reduce spatial dimensions\n",
    "        self.gap = TimeDistributed(nn.AdaptiveAvgPool2d(1))\n",
    "        \n",
    "        # Transformer encoder for temporal modeling\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=channels[-1],\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=channels[-1]*4,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
    "        \n",
    "        # Temporal attention\n",
    "        self.temporal_attention = TemporalAttention(channels[-1])\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Upsampling to original resolution\n",
    "        self.upsampling = nn.Sequential(\n",
    "            nn.ConvTranspose2d(channels[-1], channels[-2], kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(channels[-2], channels[-3], kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(channels[-3], n_output_channels, kernel_size=4, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time_steps, channels, height, width]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Apply initial convolution\n",
    "        x = self.initial_conv(x)\n",
    "        \n",
    "        # Apply residual blocks\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        x = self.spatial_attention(x)\n",
    "        \n",
    "        # Global average pooling\n",
    "        x = self.gap(x)\n",
    "        x = x.view(batch_size, self.seq_length, -1)  # [batch, time_steps, channels]\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Apply temporal attention to focus on important time steps\n",
    "        x = self.temporal_attention(x)  # [batch, channels]\n",
    "        \n",
    "        # Reshape for upsampling\n",
    "        x = x.view(batch_size, -1, 1, 1)\n",
    "        \n",
    "        # Apply dropout\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Upsampling to get final prediction\n",
    "        x = self.upsampling(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=kernel_size // 2)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride), nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.skip(identity)\n",
    "        return self.relu(out)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, n_input_channels, n_output_channels, kernel_size=3, init_dim=64, depth=4, dropout_rate=0.2):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, init_dim, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm2d(init_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        current_dim = init_dim\n",
    "        for i in range(depth):\n",
    "            out_dim = current_dim * 2 if i < depth - 1 else current_dim\n",
    "            self.res_blocks.append(ResidualBlock(current_dim, out_dim))\n",
    "            if i < depth - 1:\n",
    "                current_dim *= 2\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(dropout_rate)\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(current_dim, current_dim // 2, kernel_size=kernel_size, padding=kernel_size // 2),\n",
    "            nn.BatchNorm2d(current_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(current_dim // 2, n_output_channels, kernel_size=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "        return self.final(self.dropout(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📐 Normalizer: Z-Score Scaling for Climate Inputs & Outputs\n",
    "\n",
    "This class handles **Z-score normalization**, a crucial preprocessing step for stable and efficient neural network training:\n",
    "\n",
    "- **`set_input_statistics(mean, std)` / `set_output_statistics(...)`**: Store the mean and standard deviation computed from the training data for later use.\n",
    "- **`normalize(data, data_type)`**: Standardizes the data using `(x - mean) / std`. This is applied separately to inputs and outputs.\n",
    "- **`inverse_transform_output(data)`**: Converts model predictions back to the original physical units (e.g., Kelvin for temperature, mm/day for precipitation).\n",
    "\n",
    "Normalizing the data ensures the model sees inputs with similar dynamic ranges and avoids biases caused by different variable scales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    def __init__(self):\n",
    "        self.mean_in, self.std_in = None, None\n",
    "        self.mean_out, self.std_out = None, None\n",
    "\n",
    "    def set_input_statistics(self, mean, std):\n",
    "        self.mean_in = mean\n",
    "        self.std_in = std\n",
    "\n",
    "    def set_output_statistics(self, mean, std):\n",
    "        self.mean_out = mean\n",
    "        self.std_out = std\n",
    "\n",
    "    def normalize(self, data, data_type):\n",
    "        if data_type == \"input\":\n",
    "            return (data - self.mean_in) / self.std_in\n",
    "        elif data_type == \"output\":\n",
    "            return (data - self.mean_out) / self.std_out\n",
    "\n",
    "    def inverse_transform_output(self, data):\n",
    "        return data * self.std_out + self.mean_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌍 Data Module: Loading, Normalization, and Splitting\n",
    "\n",
    "This section handles the entire data pipeline, from loading the `.zarr` dataset to preparing PyTorch-ready DataLoaders.\n",
    "\n",
    "#### `ClimateDataset`\n",
    "- A simple PyTorch `Dataset` wrapper that preloads the entire (normalized) dataset into memory using Dask.\n",
    "- Converts the data to PyTorch tensors and handles any `NaN` checks up front.\n",
    "\n",
    "#### `ClimateDataModule`\n",
    "A PyTorch Lightning `DataModule` that handles:\n",
    "- ✅ **Loading data** from different SSP scenarios and ensemble members\n",
    "- ✅ **Broadcasting non-spatial inputs** (like CO₂) to match spatial grid size\n",
    "- ✅ **Normalization** using mean/std computed from training data only\n",
    "- ✅ **Splitting** into training, validation, and test sets:\n",
    "  - Training: All months from selected SSPs (except last 10 years of SSP370)\n",
    "  - Validation: Last 10 years (120 months) of SSP370\n",
    "  - Test: Last 10 years of SSP245 (unseen scenario)\n",
    "- ✅ **Batching** and parallelized data loading via PyTorch `DataLoader`s\n",
    "- ✅ **Latitude-based area weighting** for fair climate metric evaluation\n",
    "- Shape of the inputs are Batch_Size X 5 (num_input_variables) X 48 X 72\n",
    "- Shape of ouputputs are Batch_Size X 2 (num_output_variables) X 48 X 72\n",
    "\n",
    "> ℹ️ **Note:** You likely won’t need to modify this class but feel free to make modifications if you want to inlcude different ensemble members to feed more data to your models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClimateDataset(Dataset):\n",
    "    def __init__(self, inputs_dask, outputs_dask, output_is_normalized=True):\n",
    "        self.size = inputs_dask.shape[0]\n",
    "        print(f\"Creating dataset with {self.size} samples...\")\n",
    "\n",
    "        inputs_np = inputs_dask.compute()\n",
    "        outputs_np = outputs_dask.compute()\n",
    "\n",
    "        self.inputs = torch.from_numpy(inputs_np).float()\n",
    "        self.outputs = torch.from_numpy(outputs_np).float()\n",
    "\n",
    "        if torch.isnan(self.inputs).any() or torch.isnan(self.outputs).any():\n",
    "            raise ValueError(\"NaNs found in dataset\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.outputs[idx]\n",
    "\n",
    "\n",
    "class ClimateDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        path,\n",
    "        input_vars,\n",
    "        output_vars,\n",
    "        train_ssps,\n",
    "        test_ssp,\n",
    "        target_member_id,\n",
    "        val_split=0.1,\n",
    "        test_months=360,\n",
    "        batch_size=32,\n",
    "        num_workers=0,\n",
    "        seed=42,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        self.input_vars = input_vars\n",
    "        self.output_vars = output_vars\n",
    "        self.train_ssps = train_ssps\n",
    "        self.test_ssp = test_ssp\n",
    "        self.target_member_id = target_member_id\n",
    "        self.val_split = val_split\n",
    "        self.test_months = test_months\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.seed = seed\n",
    "        self.normalizer = Normalizer()\n",
    "\n",
    "    def prepare_data(self):\n",
    "        assert os.path.exists(self.path), f\"Data path not found: {self.path}\"\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        ds = xr.open_zarr(self.path, consolidated=False, chunks={\"time\": 24})\n",
    "        spatial_template = ds[\"rsdt\"].isel(time=0, ssp=0, drop=True)\n",
    "\n",
    "        def load_ssp(ssp):\n",
    "            input_dask, output_dask = [], []\n",
    "            for var in self.input_vars:\n",
    "                da_var = ds[var].sel(ssp=ssp)\n",
    "                if \"latitude\" in da_var.dims:\n",
    "                    da_var = da_var.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n",
    "                if \"member_id\" in da_var.dims:\n",
    "                    da_var = da_var.sel(member_id=self.target_member_id)\n",
    "                if set(da_var.dims) == {\"time\"}:\n",
    "                    da_var = da_var.broadcast_like(spatial_template).transpose(\"time\", \"y\", \"x\")\n",
    "                input_dask.append(da_var.data)\n",
    "\n",
    "            for var in self.output_vars:\n",
    "                da_out = ds[var].sel(ssp=ssp, member_id=self.target_member_id)\n",
    "                if \"latitude\" in da_out.dims:\n",
    "                    da_out = da_out.rename({\"latitude\": \"y\", \"longitude\": \"x\"})\n",
    "                output_dask.append(da_out.data)\n",
    "\n",
    "            return da.stack(input_dask, axis=1), da.stack(output_dask, axis=1)\n",
    "\n",
    "        train_input, train_output, val_input, val_output = [], [], None, None\n",
    "\n",
    "        for ssp in self.train_ssps:\n",
    "            x, y = load_ssp(ssp)\n",
    "            if ssp == \"ssp370\":\n",
    "                val_input = x[-self.test_months:]\n",
    "                val_output = y[-self.test_months:]\n",
    "                train_input.append(x[:-self.test_months])\n",
    "                train_output.append(y[:-self.test_months])\n",
    "            else:\n",
    "                train_input.append(x)\n",
    "                train_output.append(y)\n",
    "\n",
    "        train_input = da.concatenate(train_input, axis=0)\n",
    "        train_output = da.concatenate(train_output, axis=0)\n",
    "\n",
    "        self.normalizer.set_input_statistics(\n",
    "            mean=da.nanmean(train_input, axis=(0, 2, 3), keepdims=True).compute(),\n",
    "            std=da.nanstd(train_input, axis=(0, 2, 3), keepdims=True).compute(),\n",
    "        )\n",
    "        self.normalizer.set_output_statistics(\n",
    "            mean=da.nanmean(train_output, axis=(0, 2, 3), keepdims=True).compute(),\n",
    "            std=da.nanstd(train_output, axis=(0, 2, 3), keepdims=True).compute(),\n",
    "        )\n",
    "\n",
    "        train_input_norm = self.normalizer.normalize(train_input, \"input\")\n",
    "        train_output_norm = self.normalizer.normalize(train_output, \"output\")\n",
    "        val_input_norm = self.normalizer.normalize(val_input, \"input\")\n",
    "        val_output_norm = self.normalizer.normalize(val_output, \"output\")\n",
    "\n",
    "        test_input, test_output = load_ssp(self.test_ssp)\n",
    "        test_input = test_input[-self.test_months:]\n",
    "        test_output = test_output[-self.test_months:]\n",
    "        test_input_norm = self.normalizer.normalize(test_input, \"input\")\n",
    "\n",
    "        self.train_dataset = ClimateDataset(train_input_norm, train_output_norm)\n",
    "        self.val_dataset = ClimateDataset(val_input_norm, val_output_norm)\n",
    "        self.test_dataset = ClimateDataset(test_input_norm, test_output, output_is_normalized=False)\n",
    "\n",
    "        self.lat = spatial_template.y.values\n",
    "        self.lon = spatial_template.x.values\n",
    "        self.area_weights = xr.DataArray(get_lat_weights(self.lat), dims=[\"y\"], coords={\"y\": self.lat})\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n",
    "                          num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False,\n",
    "                          num_workers=self.num_workers, pin_memory=True)\n",
    "\n",
    "    def get_lat_weights(self):\n",
    "        return self.area_weights\n",
    "\n",
    "    def get_coords(self):\n",
    "        return self.lat, self.lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚡ ClimateEmulationModule: Lightning Wrapper for Climate Model Emulation\n",
    "\n",
    "This is the core model wrapper built with **PyTorch Lightning**, which organizes the training, validation, and testing logic for the climate emulation task. Lightning abstracts away much of the boilerplate code in PyTorch-based deep learning workflows, making it easier to scale models.\n",
    "\n",
    "#### ✅ Key Features\n",
    "\n",
    "- **`training_step` / `validation_step` / `test_step`**: Standard Lightning hooks for computing loss and predictions at each stage. The loss used is **Mean Squared Error (MSE)**.\n",
    "\n",
    "- **Normalization-aware outputs**:\n",
    "  - During validation and testing, predictions and targets are denormalized before evaluation using stored mean/std statistics.\n",
    "  - This ensures evaluation is done in real-world units (Kelvin and mm/day).\n",
    "\n",
    "- **Metric Evaluation** via `_evaluate()`:\n",
    "  For each variable (`tas`, `pr`), it calculates:\n",
    "  - **Monthly Area-Weighted RMSE**\n",
    "  - **Time-Mean RMSE** (RMSE on 10-year average's)\n",
    "  - **Time-Stddev MAE** (MAE on 10-year standard deviation; a measure of temporal variability)\n",
    "    \n",
    "  These metrics reflect the competition's evaluation criteria and are logged and printed.\n",
    "\n",
    "- **Kaggle Submission Writer**:\n",
    "  After testing, predictions are saved to a `.csv` file in the required Kaggle format via `_save_submission()`.\n",
    "\n",
    "- **Saving Predictions for Visualization**:\n",
    "  - Validation predictions are saved tao `val_preds.npy` and `val_trues.npy`\n",
    "  - These can be loaded later for visual inspection of the model's performance.\n",
    "\n",
    " 🔧 **Feel free to modify any part of this module** (loss functions, evaluation, training logic) to better suit your model or training pipeline / Use pure PyTorch etc.\n",
    "\n",
    "⚠️ The **final submission `.csv` file must strictly follow the format and naming convention used in `_save_submission()`**, as these `ID`s are used to match predictions to the hidden test set during evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class ClimateEmulationModule(pl.LightningModule):\n",
    "    def __init__(self, model, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.save_hyperparameters(ignore=['model']) # Save all hyperparameters except the model to self.hparams.<param_name>\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.normalizer = None\n",
    "        self.val_preds, self.val_targets = [], []\n",
    "        self.test_preds, self.test_targets = [], []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def on_fit_start(self):\n",
    "        self.normalizer = self.trainer.datamodule.normalizer  # Get the normalizer from the datamodule (see above)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch # Unpack inputs and targets (this is the output of the _getitem_ method in the Dataset above)\n",
    "        y_hat = self(x)   # Forward pass\n",
    "        loss = self.criterion(y_hat, y)  # Calculate loss\n",
    "        self.log(\"train/loss\", loss)  # Log loss for tracking\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        self.log(\"val/loss\", loss)\n",
    "\n",
    "        y_hat_np = self.normalizer.inverse_transform_output(y_hat.detach().cpu().numpy())\n",
    "        y_np = self.normalizer.inverse_transform_output(y.detach().cpu().numpy())\n",
    "        self.val_preds.append(y_hat_np)\n",
    "        self.val_targets.append(y_np)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        # Concatenate all predictions and ground truths from each val step/batch into one array\n",
    "        preds = np.concatenate(self.val_preds, axis=0)\n",
    "        trues = np.concatenate(self.val_targets, axis=0)\n",
    "        self._evaluate(preds, trues, phase=\"val\")\n",
    "        np.save(\"val_preds.npy\", preds)\n",
    "        np.save(\"val_trues.npy\", trues)\n",
    "        self.val_preds.clear()\n",
    "        self.val_targets.clear()\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_hat_np = self.normalizer.inverse_transform_output(y_hat.detach().cpu().numpy())\n",
    "        y_np = y.detach().cpu().numpy()\n",
    "        self.test_preds.append(y_hat_np)\n",
    "        self.test_targets.append(y_np)\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        # Concatenate all predictions and ground truths from each test step/batch into one array\n",
    "        preds = np.concatenate(self.test_preds, axis=0)\n",
    "        trues = np.concatenate(self.test_targets, axis=0)\n",
    "        self._evaluate(preds, trues, phase=\"test\")\n",
    "        self._save_submission(preds)\n",
    "        self.test_preds.clear()\n",
    "        self.test_targets.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "    def _evaluate(self, preds, trues, phase=\"val\"):\n",
    "        datamodule = self.trainer.datamodule\n",
    "        area_weights = datamodule.get_lat_weights()\n",
    "        lat, lon = datamodule.get_coords()\n",
    "        time = np.arange(preds.shape[0])\n",
    "        output_vars = datamodule.output_vars\n",
    "\n",
    "        for i, var in enumerate(output_vars):\n",
    "            p = preds[:, i]\n",
    "            t = trues[:, i]\n",
    "            p_xr = xr.DataArray(p, dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n",
    "            t_xr = xr.DataArray(t, dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n",
    "\n",
    "            # RMSE\n",
    "            rmse = np.sqrt(((p_xr - t_xr) ** 2).weighted(area_weights).mean((\"time\", \"y\", \"x\")).item())\n",
    "            # RMSE of time-mean\n",
    "            mean_rmse = np.sqrt(((p_xr.mean(\"time\") - t_xr.mean(\"time\")) ** 2).weighted(area_weights).mean((\"y\", \"x\")).item())\n",
    "            # MAE of time-stddev\n",
    "            std_mae = np.abs(p_xr.std(\"time\") - t_xr.std(\"time\")).weighted(area_weights).mean((\"y\", \"x\")).item()\n",
    "\n",
    "            print(f\"[{phase.upper()}] {var}: RMSE={rmse:.4f}, Time-Mean RMSE={mean_rmse:.4f}, Time-Stddev MAE={std_mae:.4f}\")\n",
    "            self.log_dict({\n",
    "                f\"{phase}/{var}/rmse\": rmse,\n",
    "                f\"{phase}/{var}/time_mean_rmse\": mean_rmse,\n",
    "                f\"{phase}/{var}/time_std_mae\": std_mae,\n",
    "            })\n",
    "\n",
    "    def _save_submission(self, predictions):\n",
    "        datamodule = self.trainer.datamodule\n",
    "        lat, lon = datamodule.get_coords()\n",
    "        output_vars = datamodule.output_vars\n",
    "        time = np.arange(predictions.shape[0])\n",
    "\n",
    "        rows = []\n",
    "        for t_idx, t in enumerate(time):\n",
    "            for var_idx, var in enumerate(output_vars):\n",
    "                for y_idx, y in enumerate(lat):\n",
    "                    for x_idx, x in enumerate(lon):\n",
    "                        row_id = f\"t{t_idx:03d}_{var}_{y:.2f}_{x:.2f}\"\n",
    "                        pred = predictions[t_idx, var_idx, y_idx, x_idx]\n",
    "                        rows.append({\"ID\": row_id, \"Prediction\": pred})\n",
    "\n",
    "        df = pd.DataFrame(rows)\n",
    "        os.makedirs(\"submissions\", exist_ok=True)\n",
    "        filepath = f\"submissions/kaggle_submission_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        df.to_csv(filepath, index=False)\n",
    "        print(f\"✅ Submission saved to: {filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚡ Training & Evaluation with PyTorch Lightning\n",
    "\n",
    "This block sets up and runs the training and testing pipeline using **PyTorch Lightning’s `Trainer`**, which abstracts away much of the boilerplate in deep learning workflows.\n",
    "\n",
    "- **Modular Setup**:\n",
    "  - `datamodule`: Handles loading, normalization, and batching of climate data.\n",
    "  - `model`: A convolutional neural network that maps climate forcings to predicted outputs.\n",
    "  - `lightning_module`: Wraps the model with training/validation/test logic and metric evaluation.\n",
    "\n",
    "- **Trainer Flexibility**:\n",
    "  The `Trainer` accepts a wide range of configuration options from `config[\"trainer\"]`, including:\n",
    "  - Number of epochs\n",
    "  - Precision (e.g., 16-bit or 32-bit)\n",
    "  - Device configuration (CPU, GPU, or TPU)\n",
    "  - Determinism, logging, callbacks, and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_3 import SequentialClimateCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import pandas as pd\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\"logs/\", name=\"seqcnn_30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wcw003/.local/lib/python3.11/site-packages/zarr/core/group.py:3301: UserWarning: Object at .DS_Store is not recognized as a component of a Zarr hierarchy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 2703 samples...\n",
      "Creating dataset with 360 samples...\n",
      "Creating dataset with 360 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                 | Params | Mode \n",
      "-----------------------------------------------------------\n",
      "0 | model     | SequentialClimateCNN | 38.7 M | train\n",
      "1 | criterion | MSELoss              | 0      | train\n",
      "-----------------------------------------------------------\n",
      "38.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "38.7 M    Total params\n",
      "154.911   Total estimated model params size (MB)\n",
      "100       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc8849307b342c190dd2e061f4190f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from models import SequentialClimateCNN\n",
    "\n",
    "datamodule = ClimateDataModule(**config[\"data\"])\n",
    "model = SequentialClimateCNN(\n",
    "    n_input_channels=len(config[\"data\"][\"input_vars\"]),\n",
    "    n_output_channels=len(config[\"data\"][\"output_vars\"]),\n",
    "    seq_length=config[\"data\"].get(\"seq_length\", 12),\n",
    "    kernel_size=config[\"model\"].get(\"kernel_size\", 5),\n",
    "    hidden_dim=config[\"model\"].get(\"init_dim\", 128),\n",
    "    spatial_depth=config[\"model\"].get(\"depth\", 5),\n",
    "    dropout_rate=config[\"model\"].get(\"dropout_rate\", 0.3)\n",
    ")\n",
    "lightning_module = ClimateEmulationModule(model, learning_rate=config[\"training\"][\"lr\"])\n",
    "\n",
    "trainer = pl.Trainer(**config[\"trainer\"], logger=csv_logger)\n",
    "trainer.fit(lightning_module, datamodule=datamodule) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_2 import ClimateTransformerCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TimeDistributed(nn.Module):\n",
    "    \"\"\"Applies a module over multiple time steps\"\"\"\n",
    "    def __init__(self, module):\n",
    "        super(TimeDistributed, self).__init__()\n",
    "        self.module = module\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape to (batch_size * time_steps, ...)\n",
    "        batch_size, time_steps = x.size(0), x.size(1)\n",
    "        x_reshaped = x.contiguous().view(batch_size * time_steps, *x.size()[2:])\n",
    "        \n",
    "        # Apply module\n",
    "        y = self.module(x_reshaped)\n",
    "        \n",
    "        # Reshape back\n",
    "        return y.contiguous().view(batch_size, time_steps, *y.size()[1:])\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial attention mechanism for focusing on important regions\"\"\"\n",
    "    def __init__(self, in_channels):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, 1, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Generate attention map\n",
    "        attention = torch.sigmoid(self.conv(x))\n",
    "        return x * attention\n",
    "\n",
    "class TemporalAttention(nn.Module):\n",
    "    \"\"\"Temporal attention mechanism for focusing on important time steps\"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch, time_steps, features]\n",
    "        # Generate attention weights\n",
    "        attention_weights = F.softmax(self.fc(x), dim=1)\n",
    "        # Apply attention\n",
    "        context = torch.sum(x * attention_weights, dim=1)\n",
    "        return context\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with larger kernels for climate patterns\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=5, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, \n",
    "                              stride=stride, padding=kernel_size//2)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, \n",
    "                              padding=kernel_size//2)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Skip connection\n",
    "        self.skip = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.skip = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.act(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.skip(identity)\n",
    "        return self.act(out)\n",
    "\n",
    "class TimeSeriesClimateCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Time Series CNN for climate emulation\n",
    "    Combines temporal processing with spatial CNN features\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_input_channels, \n",
    "        n_output_channels, \n",
    "        seq_length=12,\n",
    "        kernel_size=5, \n",
    "        init_dim=64, \n",
    "        depth=4, \n",
    "        dropout_rate=0.3,\n",
    "        use_lstm=True\n",
    "    ):\n",
    "        super(TimeSeriesClimateCNN, self).__init__()\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.use_lstm = use_lstm\n",
    "        \n",
    "        # Initial convolution applied to each time step\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, init_dim, kernel_size=kernel_size, \n",
    "                     padding=kernel_size//2),\n",
    "            nn.BatchNorm2d(init_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Time-distributed CNN blocks\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        current_dim = init_dim\n",
    "        \n",
    "        for i in range(depth):\n",
    "            out_dim = current_dim * 2 if i < depth - 1 else current_dim\n",
    "            self.res_blocks.append(\n",
    "                TimeDistributed(ResidualBlock(current_dim, out_dim))\n",
    "            )\n",
    "            if i < depth - 1:\n",
    "                current_dim *= 2\n",
    "        \n",
    "        # Spatial attention after CNN processing\n",
    "        self.spatial_attention = TimeDistributed(SpatialAttention(current_dim))\n",
    "        \n",
    "        # Global average pooling to reduce spatial dimensions\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # LSTM for temporal processing\n",
    "        if use_lstm:\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=current_dim,\n",
    "                hidden_size=current_dim*2,\n",
    "                num_layers=2,\n",
    "                batch_first=True,\n",
    "                dropout=dropout_rate if depth > 1 else 0\n",
    "            )\n",
    "            lstm_output_dim = current_dim*2\n",
    "        else:\n",
    "            lstm_output_dim = current_dim\n",
    "            \n",
    "        # Temporal attention\n",
    "        self.temporal_attention = TemporalAttention(lstm_output_dim)\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(lstm_output_dim, current_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(current_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            nn.Conv2d(current_dim, current_dim//2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(current_dim//2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(current_dim//2, n_output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Instead of trying to reshape the input based on sequence length,\n",
    "        # we'll adjust our approach to handle the actual input format\n",
    "        \n",
    "        # The input has 5 channels, which suggests it's a single time step with 5 features\n",
    "        # Let's replicate this input across the sequence length\n",
    "        \n",
    "        # Remove the debug print in production code\n",
    "        # print(f\"Input shape: {x.shape}, Total elements: {x.numel()}\")\n",
    "        \n",
    "        # Apply initial convolution directly\n",
    "        x = self.initial_conv(x)\n",
    "        \n",
    "        # Reshape to add the sequence dimension\n",
    "        # [batch_size, features, height, width] -> [batch_size, seq_length, features, height, width]\n",
    "        x = x.unsqueeze(1).repeat(1, self.seq_length, 1, 1, 1)\n",
    "        \n",
    "        # Apply residual blocks with time distribution\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        x = self.spatial_attention(x)\n",
    "        \n",
    "        # Global average pooling to reduce spatial dimensions\n",
    "        x = x.view(batch_size * self.seq_length, -1, height, width)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(batch_size, self.seq_length, -1)\n",
    "        \n",
    "        # Apply LSTM for temporal processing\n",
    "        if self.use_lstm:\n",
    "            x, _ = self.lstm(x)\n",
    "        \n",
    "        # Apply temporal attention to focus on important time steps\n",
    "        x = self.temporal_attention(x)\n",
    "        \n",
    "        # Reshape for decoder\n",
    "        x = x.view(batch_size, -1, 1, 1)\n",
    "        \n",
    "        # Expand spatially to match original dimensions\n",
    "        x = x.expand(-1, -1, height, width)\n",
    "        \n",
    "        # Final prediction\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ClimateTransformerCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer-based model for climate emulation\n",
    "    Combines self-attention for temporal processing with CNN for spatial features\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_input_channels, \n",
    "        n_output_channels, \n",
    "        seq_length=12,\n",
    "        kernel_size=5, \n",
    "        init_dim=64, \n",
    "        depth=4, \n",
    "        n_heads=8,\n",
    "        dropout_rate=0.3\n",
    "    ):\n",
    "        super(ClimateTransformerCNN, self).__init__()\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "        # Initial convolution applied to each time step\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, init_dim, kernel_size=kernel_size, \n",
    "                     padding=kernel_size//2),\n",
    "            nn.BatchNorm2d(init_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Time-distributed CNN blocks\n",
    "        self.res_blocks = nn.ModuleList()\n",
    "        current_dim = init_dim\n",
    "        \n",
    "        for i in range(depth):\n",
    "            out_dim = current_dim * 2 if i < depth - 1 else current_dim\n",
    "            self.res_blocks.append(\n",
    "                TimeDistributed(ResidualBlock(current_dim, out_dim))\n",
    "            )\n",
    "            if i < depth - 1:\n",
    "                current_dim *= 2\n",
    "        \n",
    "        # Spatial attention\n",
    "        self.spatial_attention = TimeDistributed(SpatialAttention(current_dim))\n",
    "        \n",
    "        # Global average pooling to reduce spatial dimensions\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Positional encoding for transformer\n",
    "        self.pos_encoder = nn.Parameter(\n",
    "            torch.zeros(1, 12, current_dim)\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder layer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=current_dim,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=current_dim*4,\n",
    "            dropout=dropout_rate,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer, \n",
    "            num_layers=2\n",
    "        )\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Conv2d(current_dim, current_dim, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(current_dim),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(dropout_rate),\n",
    "            nn.Conv2d(current_dim, current_dim//2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(current_dim//2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(current_dim//2, n_output_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        \n",
    "        # Instead of trying to reshape the input based on sequence length,\n",
    "        # we'll adjust our approach to handle the actual input format\n",
    "        \n",
    "        # The input has 5 channels, which suggests it's a single time step with 5 features\n",
    "        # Let's replicate this input across the sequence length\n",
    "        \n",
    "        # Remove the debug print in production code\n",
    "        # print(f\"Input shape: {x.shape}, Total elements: {x.numel()}\")\n",
    "        \n",
    "        # Apply initial convolution directly\n",
    "        x = self.initial_conv(x)\n",
    "        \n",
    "        # Reshape to add the sequence dimension\n",
    "        # [batch_size, features, height, width] -> [batch_size, seq_length, features, height, width]\n",
    "        x = x.unsqueeze(1).repeat(1, self.seq_length, 1, 1, 1)\n",
    "        \n",
    "        # Apply residual blocks with time distribution\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x)\n",
    "        \n",
    "        # Apply spatial attention\n",
    "        x = self.spatial_attention(x)\n",
    "        \n",
    "        # Global average pooling to reduce spatial dimensions\n",
    "        x = x.view(batch_size * self.seq_length, -1, height, width)\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = x.view(batch_size, self.seq_length, -1)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        if self.pos_encoder.size(1) != self.seq_length:\n",
    "            self.pos_encoder = nn.Parameter(\n",
    "                torch.zeros(1, self.seq_length, self.pos_encoder.size(2))\n",
    "            )\n",
    "        x = x + self.pos_encoder\n",
    "        \n",
    "        # Apply transformer encoder\n",
    "        x = self.transformer_encoder(x)\n",
    "        \n",
    "        # Take the last time step's output\n",
    "        x = x[:, -1]\n",
    "        \n",
    "        # Reshape for decoder\n",
    "        x = x.view(batch_size, -1, 1, 1)\n",
    "        \n",
    "        # Expand spatially to match original dimensions\n",
    "        x = x.expand(-1, -1, height, width)\n",
    "        \n",
    "        # Final prediction\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import CSVLogger\n",
    "import pandas as pd\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\"logs/\", name=\"my_experiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wcw003/.local/lib/python3.11/site-packages/zarr/core/group.py:3301: UserWarning: Object at .DS_Store is not recognized as a component of a Zarr hierarchy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 2703 samples...\n",
      "Creating dataset with 360 samples...\n",
      "Creating dataset with 360 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                  | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | model     | ClimateTransformerCNN | 9.0 M  | train\n",
      "1 | criterion | MSELoss               | 0      | train\n",
      "------------------------------------------------------------\n",
      "9.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.0 M     Total params\n",
      "36.086    Total estimated model params size (MB)\n",
      "80        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "205b2a4165c74f6180c38ca52ffa27b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.2211, Time-Mean RMSE=14.3743, Time-Stddev MAE=2.6552\n",
      "[VAL] pr: RMSE=3.5116, Time-Mean RMSE=2.1751, Time-Stddev MAE=2.1041\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.2506, Time-Mean RMSE=14.4203, Time-Stddev MAE=3.5068\n",
      "[VAL] pr: RMSE=3.5042, Time-Mean RMSE=2.1633, Time-Stddev MAE=2.1363\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.4388, Time-Mean RMSE=14.6537, Time-Stddev MAE=2.4050\n",
      "[VAL] pr: RMSE=3.5102, Time-Mean RMSE=2.1713, Time-Stddev MAE=2.0342\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.5194, Time-Mean RMSE=14.6931, Time-Stddev MAE=2.2785\n",
      "[VAL] pr: RMSE=3.5146, Time-Mean RMSE=2.1802, Time-Stddev MAE=2.0990\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.4642, Time-Mean RMSE=14.6861, Time-Stddev MAE=2.3902\n",
      "[VAL] pr: RMSE=3.5144, Time-Mean RMSE=2.1800, Time-Stddev MAE=2.1194\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.4194, Time-Mean RMSE=14.6226, Time-Stddev MAE=2.2899\n",
      "[VAL] pr: RMSE=3.5087, Time-Mean RMSE=2.1713, Time-Stddev MAE=2.0991\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.2111, Time-Mean RMSE=14.4093, Time-Stddev MAE=2.2916\n",
      "[VAL] pr: RMSE=3.5095, Time-Mean RMSE=2.1721, Time-Stddev MAE=2.1113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.8556, Time-Mean RMSE=15.0964, Time-Stddev MAE=2.2993\n",
      "[VAL] pr: RMSE=3.5195, Time-Mean RMSE=2.1871, Time-Stddev MAE=2.0450\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.3056, Time-Mean RMSE=14.5225, Time-Stddev MAE=2.3407\n",
      "[VAL] pr: RMSE=3.5133, Time-Mean RMSE=2.1785, Time-Stddev MAE=2.0876\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.6152, Time-Mean RMSE=14.8268, Time-Stddev MAE=2.6200\n",
      "[VAL] pr: RMSE=3.5200, Time-Mean RMSE=2.1895, Time-Stddev MAE=2.1001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.6927, Time-Mean RMSE=14.9163, Time-Stddev MAE=2.3118\n",
      "[VAL] pr: RMSE=3.5128, Time-Mean RMSE=2.1777, Time-Stddev MAE=2.0949\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.4608, Time-Mean RMSE=14.6794, Time-Stddev MAE=2.3100\n",
      "[VAL] pr: RMSE=3.5150, Time-Mean RMSE=2.1810, Time-Stddev MAE=2.0675\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.9856, Time-Mean RMSE=15.2497, Time-Stddev MAE=2.4291\n",
      "[VAL] pr: RMSE=3.5167, Time-Mean RMSE=2.1840, Time-Stddev MAE=2.1085\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.5908, Time-Mean RMSE=14.8272, Time-Stddev MAE=2.6516\n",
      "[VAL] pr: RMSE=3.5110, Time-Mean RMSE=2.1750, Time-Stddev MAE=2.0854\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=15.8884, Time-Mean RMSE=15.1008, Time-Stddev MAE=3.1245\n",
      "[VAL] pr: RMSE=3.5203, Time-Mean RMSE=2.1896, Time-Stddev MAE=2.1073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    }
   ],
   "source": [
    "# from models import SequentialClimateCNN\n",
    "\n",
    "datamodule = ClimateDataModule(**config[\"data\"])\n",
    "model = ClimateTransformerCNN(\n",
    "    n_input_channels=len(config[\"data\"][\"input_vars\"]),\n",
    "    n_output_channels=len(config[\"data\"][\"output_vars\"]),\n",
    "    seq_length=config[\"data\"].get(\"seq_length\", 12),\n",
    "    kernel_size=config[\"model\"].get(\"kernel_size\", 5),\n",
    "    dropout_rate=config[\"model\"].get(\"dropout_rate\", 0.3),\n",
    "    init_dim=32\n",
    ")\n",
    "lightning_module = ClimateEmulationModule(model, learning_rate=config[\"training\"][\"lr\"])\n",
    "\n",
    "trainer = pl.Trainer(**config[\"trainer\"], logger = csv_logger)\n",
    "trainer.fit(lightning_module, datamodule=datamodule) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>val/loss</th>\n",
       "      <th>val/pr/rmse</th>\n",
       "      <th>val/pr/time_mean_rmse</th>\n",
       "      <th>val/pr/time_std_mae</th>\n",
       "      <th>val/tas/rmse</th>\n",
       "      <th>val/tas/time_mean_rmse</th>\n",
       "      <th>val/tas/time_std_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0.906758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>0.864546</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>0.886342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>0.812010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>0.864929</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>14</td>\n",
       "      <td>4899</td>\n",
       "      <td>0.834978</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>14</td>\n",
       "      <td>4949</td>\n",
       "      <td>0.840607</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>14</td>\n",
       "      <td>4999</td>\n",
       "      <td>0.845257</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>14</td>\n",
       "      <td>5049</td>\n",
       "      <td>0.817505</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>14</td>\n",
       "      <td>5069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.853512</td>\n",
       "      <td>3.520349</td>\n",
       "      <td>2.189595</td>\n",
       "      <td>2.107254</td>\n",
       "      <td>15.888366</td>\n",
       "      <td>15.100842</td>\n",
       "      <td>3.124456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  step  train/loss  val/loss  val/pr/rmse  val/pr/time_mean_rmse  \\\n",
       "0        0    49    0.906758       NaN          NaN                    NaN   \n",
       "1        0    49    0.864546       NaN          NaN                    NaN   \n",
       "2        0    99    0.886342       NaN          NaN                    NaN   \n",
       "3        0   149    0.812010       NaN          NaN                    NaN   \n",
       "4        0   199    0.864929       NaN          NaN                    NaN   \n",
       "..     ...   ...         ...       ...          ...                    ...   \n",
       "112     14  4899    0.834978       NaN          NaN                    NaN   \n",
       "113     14  4949    0.840607       NaN          NaN                    NaN   \n",
       "114     14  4999    0.845257       NaN          NaN                    NaN   \n",
       "115     14  5049    0.817505       NaN          NaN                    NaN   \n",
       "116     14  5069         NaN  0.853512     3.520349               2.189595   \n",
       "\n",
       "     val/pr/time_std_mae  val/tas/rmse  val/tas/time_mean_rmse  \\\n",
       "0                    NaN           NaN                     NaN   \n",
       "1                    NaN           NaN                     NaN   \n",
       "2                    NaN           NaN                     NaN   \n",
       "3                    NaN           NaN                     NaN   \n",
       "4                    NaN           NaN                     NaN   \n",
       "..                   ...           ...                     ...   \n",
       "112                  NaN           NaN                     NaN   \n",
       "113                  NaN           NaN                     NaN   \n",
       "114                  NaN           NaN                     NaN   \n",
       "115                  NaN           NaN                     NaN   \n",
       "116             2.107254     15.888366               15.100842   \n",
       "\n",
       "     val/tas/time_std_mae  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "..                    ...  \n",
       "112                   NaN  \n",
       "113                   NaN  \n",
       "114                   NaN  \n",
       "115                   NaN  \n",
       "116              3.124456  \n",
       "\n",
       "[117 rows x 10 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f\"{csv_logger.log_dir}/metrics.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_logger = CSVLogger(save_dir=\"logs/\", name=\"baseline_10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/wcw003/.local/lib/python3.11/site-packages/zarr/core/group.py:3301: UserWarning: Object at .DS_Store is not recognized as a component of a Zarr hierarchy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 2703 samples...\n",
      "Creating dataset with 360 samples...\n",
      "Creating dataset with 360 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model     | SimpleCNN | 27.4 M | train\n",
      "1 | criterion | MSELoss   | 0      | train\n",
      "------------------------------------------------\n",
      "27.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.4 M    Total params\n",
      "109.507   Total estimated model params size (MB)\n",
      "47        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c537f8c59864ecda06cc2d702f2b845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=5.5959, Time-Mean RMSE=3.9459, Time-Stddev MAE=1.3635\n",
      "[VAL] pr: RMSE=3.0445, Time-Mean RMSE=1.3536, Time-Stddev MAE=1.5442\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=4.9433, Time-Mean RMSE=3.6218, Time-Stddev MAE=1.2847\n",
      "[VAL] pr: RMSE=2.8297, Time-Mean RMSE=1.0334, Time-Stddev MAE=1.5142\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=4.1288, Time-Mean RMSE=2.7496, Time-Stddev MAE=1.2022\n",
      "[VAL] pr: RMSE=2.6712, Time-Mean RMSE=0.7295, Time-Stddev MAE=1.4488\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=4.3310, Time-Mean RMSE=2.3892, Time-Stddev MAE=1.1262\n",
      "[VAL] pr: RMSE=2.6687, Time-Mean RMSE=0.6449, Time-Stddev MAE=1.4761\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=3.4989, Time-Mean RMSE=1.8603, Time-Stddev MAE=1.2495\n",
      "[VAL] pr: RMSE=2.6075, Time-Mean RMSE=0.5869, Time-Stddev MAE=1.4143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=3.5850, Time-Mean RMSE=2.1104, Time-Stddev MAE=0.9973\n",
      "[VAL] pr: RMSE=2.6450, Time-Mean RMSE=0.7606, Time-Stddev MAE=1.3286\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=3.6807, Time-Mean RMSE=2.2362, Time-Stddev MAE=0.8098\n",
      "[VAL] pr: RMSE=2.4985, Time-Mean RMSE=0.5415, Time-Stddev MAE=1.1800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.9213, Time-Mean RMSE=1.4191, Time-Stddev MAE=0.6317\n",
      "[VAL] pr: RMSE=2.2687, Time-Mean RMSE=0.5588, Time-Stddev MAE=1.0402\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.7272, Time-Mean RMSE=1.2288, Time-Stddev MAE=0.8376\n",
      "[VAL] pr: RMSE=2.1631, Time-Mean RMSE=0.4590, Time-Stddev MAE=0.9732\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.7286, Time-Mean RMSE=1.4483, Time-Stddev MAE=0.6481\n",
      "[VAL] pr: RMSE=2.1200, Time-Mean RMSE=0.4344, Time-Stddev MAE=0.9280\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.5357, Time-Mean RMSE=1.2193, Time-Stddev MAE=0.5908\n",
      "[VAL] pr: RMSE=2.1071, Time-Mean RMSE=0.4265, Time-Stddev MAE=0.8365\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.8629, Time-Mean RMSE=1.8826, Time-Stddev MAE=0.7568\n",
      "[VAL] pr: RMSE=2.1175, Time-Mean RMSE=0.4867, Time-Stddev MAE=0.8961\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.7121, Time-Mean RMSE=1.5193, Time-Stddev MAE=0.8872\n",
      "[VAL] pr: RMSE=2.0750, Time-Mean RMSE=0.4783, Time-Stddev MAE=0.8483\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.2043, Time-Mean RMSE=1.0390, Time-Stddev MAE=0.5223\n",
      "[VAL] pr: RMSE=2.0395, Time-Mean RMSE=0.3944, Time-Stddev MAE=0.8404\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] tas: RMSE=2.0285, Time-Mean RMSE=1.0015, Time-Stddev MAE=0.5026\n",
      "[VAL] pr: RMSE=2.0292, Time-Mean RMSE=0.3596, Time-Stddev MAE=0.8042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    }
   ],
   "source": [
    "#original\n",
    "datamodule = ClimateDataModule(**config[\"data\"])\n",
    "model = SimpleCNN(\n",
    "    n_input_channels=len(config[\"data\"][\"input_vars\"]),\n",
    "    n_output_channels=len(config[\"data\"][\"output_vars\"]),\n",
    "    **{k: v for k, v in config[\"model\"].items() if k != \"type\"}\n",
    ")\n",
    "lightning_module = ClimateEmulationModule(model, learning_rate=config[\"training\"][\"lr\"])\n",
    "\n",
    "trainer = pl.Trainer(**config[\"trainer\"], logger = csv_logger)\n",
    "trainer.fit(lightning_module, datamodule=datamodule)   # Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model\n",
    "\n",
    "**IMPORTANT:** Please note that the test metrics will be bad because the test targets have been corrupted on the public Kaggle dataset.\n",
    "The purpose of testing below is to generate the Kaggle submission file based on your model's predictions, which you can submit to the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wcw003/.local/lib/python3.11/site-packages/zarr/core/group.py:3301: UserWarning: Object at .DS_Store is not recognized as a component of a Zarr hierarchy.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 2703 samples...\n",
      "Creating dataset with 360 samples...\n",
      "Creating dataset with 360 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac23d1545e044cfd9df9d7d939ca6268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TEST] tas: RMSE=289.9964, Time-Mean RMSE=289.9575, Time-Stddev MAE=3.3582\n",
      "[TEST] pr: RMSE=4.1643, Time-Mean RMSE=3.7216, Time-Stddev MAE=1.3198\n",
      "✅ Submission saved to: submissions/kaggle_submission_20250516_001026.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/pr/rmse        </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     4.164330959320068     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/pr/time_mean_rmse   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     3.721630573272705     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test/pr/time_std_mae    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     1.319814682006836     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test/tas/rmse       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     289.9964599609375     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">  test/tas/time_mean_rmse  </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     289.9574890136719     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">   test/tas/time_std_mae   </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    3.3581554889678955     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/pr/rmse       \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    4.164330959320068    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/pr/time_mean_rmse  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    3.721630573272705    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test/pr/time_std_mae   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    1.319814682006836    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test/tas/rmse      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    289.9964599609375    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m test/tas/time_mean_rmse \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    289.9574890136719    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m  test/tas/time_std_mae  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   3.3581554889678955    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test/tas/rmse': 289.9964599609375,\n",
       "  'test/tas/time_mean_rmse': 289.9574890136719,\n",
       "  'test/tas/time_std_mae': 3.3581554889678955,\n",
       "  'test/pr/rmse': 4.164330959320068,\n",
       "  'test/pr/time_mean_rmse': 3.721630573272705,\n",
       "  'test/pr/time_std_mae': 1.319814682006836}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(lightning_module, datamodule=datamodule) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(f\"{csv_logger.log_dir}/metrics.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('logs/baseline_10/version_0/metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>test/pr/rmse</th>\n",
       "      <th>test/pr/time_mean_rmse</th>\n",
       "      <th>test/pr/time_std_mae</th>\n",
       "      <th>test/tas/rmse</th>\n",
       "      <th>test/tas/time_mean_rmse</th>\n",
       "      <th>test/tas/time_std_mae</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>val/loss</th>\n",
       "      <th>val/pr/rmse</th>\n",
       "      <th>val/pr/time_mean_rmse</th>\n",
       "      <th>val/pr/time_std_mae</th>\n",
       "      <th>val/tas/rmse</th>\n",
       "      <th>val/tas/time_mean_rmse</th>\n",
       "      <th>val/tas/time_std_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.413533</td>\n",
       "      <td>3.044483</td>\n",
       "      <td>1.353611</td>\n",
       "      <td>1.544218</td>\n",
       "      <td>5.595884</td>\n",
       "      <td>3.945855</td>\n",
       "      <td>1.363540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.352440</td>\n",
       "      <td>2.829718</td>\n",
       "      <td>1.033430</td>\n",
       "      <td>1.514174</td>\n",
       "      <td>4.943322</td>\n",
       "      <td>3.621779</td>\n",
       "      <td>1.284734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>1013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.310559</td>\n",
       "      <td>2.671203</td>\n",
       "      <td>0.729546</td>\n",
       "      <td>1.448786</td>\n",
       "      <td>4.128765</td>\n",
       "      <td>2.749595</td>\n",
       "      <td>1.202235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>1351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.312051</td>\n",
       "      <td>2.668728</td>\n",
       "      <td>0.644865</td>\n",
       "      <td>1.476117</td>\n",
       "      <td>4.330984</td>\n",
       "      <td>2.389239</td>\n",
       "      <td>1.126163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4</td>\n",
       "      <td>1689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.286025</td>\n",
       "      <td>2.607461</td>\n",
       "      <td>0.586888</td>\n",
       "      <td>1.414261</td>\n",
       "      <td>3.498919</td>\n",
       "      <td>1.860331</td>\n",
       "      <td>1.249455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5</td>\n",
       "      <td>2027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.290555</td>\n",
       "      <td>2.645019</td>\n",
       "      <td>0.760569</td>\n",
       "      <td>1.328599</td>\n",
       "      <td>3.585017</td>\n",
       "      <td>2.110446</td>\n",
       "      <td>0.997285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>6</td>\n",
       "      <td>2365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.266779</td>\n",
       "      <td>2.498491</td>\n",
       "      <td>0.541491</td>\n",
       "      <td>1.179986</td>\n",
       "      <td>3.680653</td>\n",
       "      <td>2.236222</td>\n",
       "      <td>0.809780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>7</td>\n",
       "      <td>2703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.217828</td>\n",
       "      <td>2.268676</td>\n",
       "      <td>0.558791</td>\n",
       "      <td>1.040210</td>\n",
       "      <td>2.921343</td>\n",
       "      <td>1.419113</td>\n",
       "      <td>0.631662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>8</td>\n",
       "      <td>3041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.197965</td>\n",
       "      <td>2.163139</td>\n",
       "      <td>0.459045</td>\n",
       "      <td>0.973214</td>\n",
       "      <td>2.727245</td>\n",
       "      <td>1.228776</td>\n",
       "      <td>0.837583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>9</td>\n",
       "      <td>3379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.189973</td>\n",
       "      <td>2.120011</td>\n",
       "      <td>0.434374</td>\n",
       "      <td>0.927952</td>\n",
       "      <td>2.728606</td>\n",
       "      <td>1.448274</td>\n",
       "      <td>0.648062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>10</td>\n",
       "      <td>3717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.186966</td>\n",
       "      <td>2.107061</td>\n",
       "      <td>0.426533</td>\n",
       "      <td>0.836507</td>\n",
       "      <td>2.535743</td>\n",
       "      <td>1.219256</td>\n",
       "      <td>0.590770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>11</td>\n",
       "      <td>4055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.192601</td>\n",
       "      <td>2.117505</td>\n",
       "      <td>0.486652</td>\n",
       "      <td>0.896061</td>\n",
       "      <td>2.862914</td>\n",
       "      <td>1.882579</td>\n",
       "      <td>0.756838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>12</td>\n",
       "      <td>4393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.182255</td>\n",
       "      <td>2.075044</td>\n",
       "      <td>0.478335</td>\n",
       "      <td>0.848311</td>\n",
       "      <td>2.712114</td>\n",
       "      <td>1.519333</td>\n",
       "      <td>0.887230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>13</td>\n",
       "      <td>4731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.173374</td>\n",
       "      <td>2.039508</td>\n",
       "      <td>0.394416</td>\n",
       "      <td>0.840359</td>\n",
       "      <td>2.204287</td>\n",
       "      <td>1.039007</td>\n",
       "      <td>0.522312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>14</td>\n",
       "      <td>5069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.170418</td>\n",
       "      <td>2.029204</td>\n",
       "      <td>0.359566</td>\n",
       "      <td>0.804228</td>\n",
       "      <td>2.028456</td>\n",
       "      <td>1.001509</td>\n",
       "      <td>0.502590</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  step  test/pr/rmse  test/pr/time_mean_rmse  test/pr/time_std_mae  \\\n",
       "6        0   337           NaN                     NaN                   NaN   \n",
       "14       1   675           NaN                     NaN                   NaN   \n",
       "22       2  1013           NaN                     NaN                   NaN   \n",
       "30       3  1351           NaN                     NaN                   NaN   \n",
       "37       4  1689           NaN                     NaN                   NaN   \n",
       "45       5  2027           NaN                     NaN                   NaN   \n",
       "53       6  2365           NaN                     NaN                   NaN   \n",
       "61       7  2703           NaN                     NaN                   NaN   \n",
       "68       8  3041           NaN                     NaN                   NaN   \n",
       "76       9  3379           NaN                     NaN                   NaN   \n",
       "84      10  3717           NaN                     NaN                   NaN   \n",
       "92      11  4055           NaN                     NaN                   NaN   \n",
       "99      12  4393           NaN                     NaN                   NaN   \n",
       "107     13  4731           NaN                     NaN                   NaN   \n",
       "115     14  5069           NaN                     NaN                   NaN   \n",
       "\n",
       "     test/tas/rmse  test/tas/time_mean_rmse  test/tas/time_std_mae  \\\n",
       "6              NaN                      NaN                    NaN   \n",
       "14             NaN                      NaN                    NaN   \n",
       "22             NaN                      NaN                    NaN   \n",
       "30             NaN                      NaN                    NaN   \n",
       "37             NaN                      NaN                    NaN   \n",
       "45             NaN                      NaN                    NaN   \n",
       "53             NaN                      NaN                    NaN   \n",
       "61             NaN                      NaN                    NaN   \n",
       "68             NaN                      NaN                    NaN   \n",
       "76             NaN                      NaN                    NaN   \n",
       "84             NaN                      NaN                    NaN   \n",
       "92             NaN                      NaN                    NaN   \n",
       "99             NaN                      NaN                    NaN   \n",
       "107            NaN                      NaN                    NaN   \n",
       "115            NaN                      NaN                    NaN   \n",
       "\n",
       "     train/loss  val/loss  val/pr/rmse  val/pr/time_mean_rmse  \\\n",
       "6           NaN  0.413533     3.044483               1.353611   \n",
       "14          NaN  0.352440     2.829718               1.033430   \n",
       "22          NaN  0.310559     2.671203               0.729546   \n",
       "30          NaN  0.312051     2.668728               0.644865   \n",
       "37          NaN  0.286025     2.607461               0.586888   \n",
       "45          NaN  0.290555     2.645019               0.760569   \n",
       "53          NaN  0.266779     2.498491               0.541491   \n",
       "61          NaN  0.217828     2.268676               0.558791   \n",
       "68          NaN  0.197965     2.163139               0.459045   \n",
       "76          NaN  0.189973     2.120011               0.434374   \n",
       "84          NaN  0.186966     2.107061               0.426533   \n",
       "92          NaN  0.192601     2.117505               0.486652   \n",
       "99          NaN  0.182255     2.075044               0.478335   \n",
       "107         NaN  0.173374     2.039508               0.394416   \n",
       "115         NaN  0.170418     2.029204               0.359566   \n",
       "\n",
       "     val/pr/time_std_mae  val/tas/rmse  val/tas/time_mean_rmse  \\\n",
       "6               1.544218      5.595884                3.945855   \n",
       "14              1.514174      4.943322                3.621779   \n",
       "22              1.448786      4.128765                2.749595   \n",
       "30              1.476117      4.330984                2.389239   \n",
       "37              1.414261      3.498919                1.860331   \n",
       "45              1.328599      3.585017                2.110446   \n",
       "53              1.179986      3.680653                2.236222   \n",
       "61              1.040210      2.921343                1.419113   \n",
       "68              0.973214      2.727245                1.228776   \n",
       "76              0.927952      2.728606                1.448274   \n",
       "84              0.836507      2.535743                1.219256   \n",
       "92              0.896061      2.862914                1.882579   \n",
       "99              0.848311      2.712114                1.519333   \n",
       "107             0.840359      2.204287                1.039007   \n",
       "115             0.804228      2.028456                1.001509   \n",
       "\n",
       "     val/tas/time_std_mae  \n",
       "6                1.363540  \n",
       "14               1.284734  \n",
       "22               1.202235  \n",
       "30               1.126163  \n",
       "37               1.249455  \n",
       "45               0.997285  \n",
       "53               0.809780  \n",
       "61               0.631662  \n",
       "68               0.837583  \n",
       "76               0.648062  \n",
       "84               0.590770  \n",
       "92               0.756838  \n",
       "99               0.887230  \n",
       "107              0.522312  \n",
       "115              0.502590  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline for 15 epochs\n",
    "df[df['val/tas/rmse'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('logs/ClimateTransformerCNN_15/version_0/metrics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>test/pr/rmse</th>\n",
       "      <th>test/pr/time_mean_rmse</th>\n",
       "      <th>test/pr/time_std_mae</th>\n",
       "      <th>test/tas/rmse</th>\n",
       "      <th>test/tas/time_mean_rmse</th>\n",
       "      <th>test/tas/time_std_mae</th>\n",
       "      <th>train/loss</th>\n",
       "      <th>val/loss</th>\n",
       "      <th>val/pr/rmse</th>\n",
       "      <th>val/pr/time_mean_rmse</th>\n",
       "      <th>val/pr/time_std_mae</th>\n",
       "      <th>val/tas/rmse</th>\n",
       "      <th>val/tas/time_mean_rmse</th>\n",
       "      <th>val/tas/time_std_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>337</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.860833</td>\n",
       "      <td>3.511637</td>\n",
       "      <td>2.175080</td>\n",
       "      <td>2.104146</td>\n",
       "      <td>15.221120</td>\n",
       "      <td>14.374287</td>\n",
       "      <td>2.655231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>675</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.861554</td>\n",
       "      <td>3.504153</td>\n",
       "      <td>2.163297</td>\n",
       "      <td>2.136274</td>\n",
       "      <td>15.250577</td>\n",
       "      <td>14.420325</td>\n",
       "      <td>3.506759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2</td>\n",
       "      <td>1013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.854624</td>\n",
       "      <td>3.510213</td>\n",
       "      <td>2.171349</td>\n",
       "      <td>2.034225</td>\n",
       "      <td>15.438791</td>\n",
       "      <td>14.653691</td>\n",
       "      <td>2.405034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>1351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.855240</td>\n",
       "      <td>3.514596</td>\n",
       "      <td>2.180201</td>\n",
       "      <td>2.099018</td>\n",
       "      <td>15.519410</td>\n",
       "      <td>14.693075</td>\n",
       "      <td>2.278483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4</td>\n",
       "      <td>1689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.852906</td>\n",
       "      <td>3.514411</td>\n",
       "      <td>2.180029</td>\n",
       "      <td>2.119374</td>\n",
       "      <td>15.464151</td>\n",
       "      <td>14.686107</td>\n",
       "      <td>2.390196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5</td>\n",
       "      <td>2027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.853600</td>\n",
       "      <td>3.508709</td>\n",
       "      <td>2.171291</td>\n",
       "      <td>2.099084</td>\n",
       "      <td>15.419410</td>\n",
       "      <td>14.622574</td>\n",
       "      <td>2.289880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>6</td>\n",
       "      <td>2365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.853228</td>\n",
       "      <td>3.509523</td>\n",
       "      <td>2.172109</td>\n",
       "      <td>2.111347</td>\n",
       "      <td>15.211109</td>\n",
       "      <td>14.409259</td>\n",
       "      <td>2.291581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>7</td>\n",
       "      <td>2703</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.851899</td>\n",
       "      <td>3.519500</td>\n",
       "      <td>2.187137</td>\n",
       "      <td>2.045025</td>\n",
       "      <td>15.855614</td>\n",
       "      <td>15.096375</td>\n",
       "      <td>2.299298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>8</td>\n",
       "      <td>3041</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.850602</td>\n",
       "      <td>3.513256</td>\n",
       "      <td>2.178537</td>\n",
       "      <td>2.087612</td>\n",
       "      <td>15.305568</td>\n",
       "      <td>14.522534</td>\n",
       "      <td>2.340683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>9</td>\n",
       "      <td>3379</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.851674</td>\n",
       "      <td>3.519984</td>\n",
       "      <td>2.189481</td>\n",
       "      <td>2.100095</td>\n",
       "      <td>15.615163</td>\n",
       "      <td>14.826754</td>\n",
       "      <td>2.620029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>10</td>\n",
       "      <td>3717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.850209</td>\n",
       "      <td>3.512788</td>\n",
       "      <td>2.177703</td>\n",
       "      <td>2.094908</td>\n",
       "      <td>15.692673</td>\n",
       "      <td>14.916316</td>\n",
       "      <td>2.311785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>11</td>\n",
       "      <td>4055</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.850446</td>\n",
       "      <td>3.514957</td>\n",
       "      <td>2.181020</td>\n",
       "      <td>2.067522</td>\n",
       "      <td>15.460815</td>\n",
       "      <td>14.679411</td>\n",
       "      <td>2.310002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>12</td>\n",
       "      <td>4393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.851225</td>\n",
       "      <td>3.516725</td>\n",
       "      <td>2.184010</td>\n",
       "      <td>2.108486</td>\n",
       "      <td>15.985601</td>\n",
       "      <td>15.249730</td>\n",
       "      <td>2.429089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>13</td>\n",
       "      <td>4731</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.850341</td>\n",
       "      <td>3.511024</td>\n",
       "      <td>2.175007</td>\n",
       "      <td>2.085424</td>\n",
       "      <td>15.590814</td>\n",
       "      <td>14.827250</td>\n",
       "      <td>2.651645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>14</td>\n",
       "      <td>5069</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.853512</td>\n",
       "      <td>3.520349</td>\n",
       "      <td>2.189595</td>\n",
       "      <td>2.107254</td>\n",
       "      <td>15.888366</td>\n",
       "      <td>15.100842</td>\n",
       "      <td>3.124456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     epoch  step  test/pr/rmse  test/pr/time_mean_rmse  test/pr/time_std_mae  \\\n",
       "7        0   337           NaN                     NaN                   NaN   \n",
       "15       1   675           NaN                     NaN                   NaN   \n",
       "23       2  1013           NaN                     NaN                   NaN   \n",
       "31       3  1351           NaN                     NaN                   NaN   \n",
       "38       4  1689           NaN                     NaN                   NaN   \n",
       "46       5  2027           NaN                     NaN                   NaN   \n",
       "54       6  2365           NaN                     NaN                   NaN   \n",
       "62       7  2703           NaN                     NaN                   NaN   \n",
       "69       8  3041           NaN                     NaN                   NaN   \n",
       "77       9  3379           NaN                     NaN                   NaN   \n",
       "85      10  3717           NaN                     NaN                   NaN   \n",
       "93      11  4055           NaN                     NaN                   NaN   \n",
       "100     12  4393           NaN                     NaN                   NaN   \n",
       "108     13  4731           NaN                     NaN                   NaN   \n",
       "116     14  5069           NaN                     NaN                   NaN   \n",
       "\n",
       "     test/tas/rmse  test/tas/time_mean_rmse  test/tas/time_std_mae  \\\n",
       "7              NaN                      NaN                    NaN   \n",
       "15             NaN                      NaN                    NaN   \n",
       "23             NaN                      NaN                    NaN   \n",
       "31             NaN                      NaN                    NaN   \n",
       "38             NaN                      NaN                    NaN   \n",
       "46             NaN                      NaN                    NaN   \n",
       "54             NaN                      NaN                    NaN   \n",
       "62             NaN                      NaN                    NaN   \n",
       "69             NaN                      NaN                    NaN   \n",
       "77             NaN                      NaN                    NaN   \n",
       "85             NaN                      NaN                    NaN   \n",
       "93             NaN                      NaN                    NaN   \n",
       "100            NaN                      NaN                    NaN   \n",
       "108            NaN                      NaN                    NaN   \n",
       "116            NaN                      NaN                    NaN   \n",
       "\n",
       "     train/loss  val/loss  val/pr/rmse  val/pr/time_mean_rmse  \\\n",
       "7           NaN  0.860833     3.511637               2.175080   \n",
       "15          NaN  0.861554     3.504153               2.163297   \n",
       "23          NaN  0.854624     3.510213               2.171349   \n",
       "31          NaN  0.855240     3.514596               2.180201   \n",
       "38          NaN  0.852906     3.514411               2.180029   \n",
       "46          NaN  0.853600     3.508709               2.171291   \n",
       "54          NaN  0.853228     3.509523               2.172109   \n",
       "62          NaN  0.851899     3.519500               2.187137   \n",
       "69          NaN  0.850602     3.513256               2.178537   \n",
       "77          NaN  0.851674     3.519984               2.189481   \n",
       "85          NaN  0.850209     3.512788               2.177703   \n",
       "93          NaN  0.850446     3.514957               2.181020   \n",
       "100         NaN  0.851225     3.516725               2.184010   \n",
       "108         NaN  0.850341     3.511024               2.175007   \n",
       "116         NaN  0.853512     3.520349               2.189595   \n",
       "\n",
       "     val/pr/time_std_mae  val/tas/rmse  val/tas/time_mean_rmse  \\\n",
       "7               2.104146     15.221120               14.374287   \n",
       "15              2.136274     15.250577               14.420325   \n",
       "23              2.034225     15.438791               14.653691   \n",
       "31              2.099018     15.519410               14.693075   \n",
       "38              2.119374     15.464151               14.686107   \n",
       "46              2.099084     15.419410               14.622574   \n",
       "54              2.111347     15.211109               14.409259   \n",
       "62              2.045025     15.855614               15.096375   \n",
       "69              2.087612     15.305568               14.522534   \n",
       "77              2.100095     15.615163               14.826754   \n",
       "85              2.094908     15.692673               14.916316   \n",
       "93              2.067522     15.460815               14.679411   \n",
       "100             2.108486     15.985601               15.249730   \n",
       "108             2.085424     15.590814               14.827250   \n",
       "116             2.107254     15.888366               15.100842   \n",
       "\n",
       "     val/tas/time_std_mae  \n",
       "7                2.655231  \n",
       "15               3.506759  \n",
       "23               2.405034  \n",
       "31               2.278483  \n",
       "38               2.390196  \n",
       "46               2.289880  \n",
       "54               2.291581  \n",
       "62               2.299298  \n",
       "69               2.340683  \n",
       "77               2.620029  \n",
       "85               2.311785  \n",
       "93               2.310002  \n",
       "100              2.429089  \n",
       "108              2.651645  \n",
       "116              3.124456  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ClimateTransformerCNN for 15 epochs\n",
    "df_1[df_1['val/tas/rmse'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(true_xr, pred_xr, title, cmap='viridis', diff_cmap='RdBu_r', metric=None):\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "    vmin = min(true_xr.min().item(), pred_xr.min().item())\n",
    "    vmax = max(true_xr.max().item(), pred_xr.max().item())\n",
    "\n",
    "    # Ground truth\n",
    "    true_xr.plot(ax=axs[0], cmap=cmap, vmin=vmin, vmax=vmax, add_colorbar=True)\n",
    "    axs[0].set_title(f\"{title} (Ground Truth)\")\n",
    "\n",
    "    # Prediction\n",
    "    pred_xr.plot(ax=axs[1], cmap=cmap, vmin=vmin, vmax=vmax, add_colorbar=True)\n",
    "    axs[1].set_title(f\"{title} (Prediction)\")\n",
    "\n",
    "    # Difference\n",
    "    diff = pred_xr - true_xr\n",
    "    abs_max = np.max(np.abs(diff))\n",
    "    diff.plot(ax=axs[2], cmap=diff_cmap, vmin=-abs_max, vmax=abs_max, add_colorbar=True)\n",
    "    axs[2].set_title(f\"{title} (Difference) {f'- {metric:.4f}' if metric else ''}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🖼️ Visualizing Validation Predictions\n",
    "\n",
    "This cell loads saved validation predictions and compares them to the ground truth using spatial plots. These visualizations help you qualitatively assess your model's performance.\n",
    "\n",
    "For each output variable (`tas`, `pr`), we visualize:\n",
    "\n",
    "- **📈 Time-Mean Map**: The 10-year average spatial pattern for both prediction and ground truth. Helps identify long-term biases or spatial shifts.\n",
    "- **📊 Time-Stddev Map**: Shows the standard deviation across time for each grid cell — useful for assessing how well the model captures **temporal variability** at each location.\n",
    "- **🕓 Random Timestep Sample**: Visual comparison of prediction vs ground truth for a single month. Useful for spotting fine-grained anomalies or errors in specific months.\n",
    "\n",
    "> These plots provide intuition beyond metrics and are useful for debugging spatial or temporal model failures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load validation predictions\n",
    "# make sure to have run the validation loop at least once\n",
    "val_preds = np.load(\"val_preds.npy\")\n",
    "val_trues = np.load(\"val_trues.npy\")\n",
    "\n",
    "lat, lon = datamodule.get_coords()\n",
    "output_vars = datamodule.output_vars\n",
    "time = np.arange(val_preds.shape[0])\n",
    "\n",
    "for i, var in enumerate(output_vars):\n",
    "    pred_xr = xr.DataArray(val_preds[:, i], dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n",
    "    true_xr = xr.DataArray(val_trues[:, i], dims=[\"time\", \"y\", \"x\"], coords={\"time\": time, \"y\": lat, \"x\": lon})\n",
    "\n",
    "    # --- Time Mean ---\n",
    "    plot_comparison(true_xr.mean(\"time\"), pred_xr.mean(\"time\"), f\"{var} Val Time-Mean\")\n",
    "\n",
    "    # --- Time Stddev ---\n",
    "    plot_comparison(true_xr.std(\"time\"), pred_xr.std(\"time\"), f\"{var} Val Time-Stddev\", cmap=\"plasma\")\n",
    "\n",
    "    # --- Random timestep ---\n",
    "    t_idx = np.random.randint(0, len(time))\n",
    "    plot_comparison(true_xr.isel(time=t_idx), pred_xr.isel(time=t_idx), f\"{var} Val Sample Timestep {t_idx}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧪 Final Notes\n",
    "\n",
    "This notebook is meant to serve as a **baseline template** — a starting point to help you get up and running quickly with the climate emulation challenge.\n",
    "\n",
    "You are **not** required to stick to this exact setup. In fact, we **encourage** you to:\n",
    "\n",
    "- 🔁 Build on top of the provided `DataModule`. \n",
    "- 🧠 Use your own model architectures or training pipelines that you’re more comfortable with \n",
    "- ⚗️ Experiment with ideas  \n",
    "- 🥇 Compete creatively to climb the Kaggle leaderboard  \n",
    "- 🙌 Most importantly: **have fun** and **learn as much as you can** along the way\n",
    "\n",
    "This challenge simulates a real-world scientific problem, and there’s no single \"correct\" approach — so be curious, experiment boldly, and make it your own!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
